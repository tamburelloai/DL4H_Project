{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Draft** (CS 598: Deep Learning For Healthcare)\n"
      ],
      "metadata": {
        "id": "FjgFx7KC3S0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# **Introduction**\n",
        "***Combining structured and unstructured data for predictive models: a deep learning approach*** (Dongdong Zhang, Changchang Yin, Jucheng Zeng, Xiaohui Yuan, and Ping Zhang) proposes an innovative approach to improve the performance of predictive models in healthcare by integrating both structured and unstructured data from Electronic Health Records (EHRs) using deep learning techniques.\n",
        "\n",
        "The authors introduce two general-purpose, multi-modal neural network architectures designed to fuse sequential unstructured notes with structured data, enhancing patient representation learning.\n",
        "\n",
        "These models employ document embeddings for long clinical note documents and utilize either convolutional neural networks (CNNs) or long short-term memory (LSTM) networks for modeling the sequential notes and temporal signals, along with one-hot encoding for static information representation. The combined data approach aims to improve the predictions of in-hospital mortality, 30-day hospital readmission, and length of stay, showing promising results over traditional models that use either type of data in isolation.\n",
        "\n",
        "The paper's contribution lies in demonstrating the efficacy of deep learning models that fuse structured and unstructured EHR data for better patient representation and improved prediction accuracy. By leveraging the complementary strengths of both data types, the proposed models achieve significant performance improvements in critical predictive tasks. This research underscores the potential of integrating heterogeneous data types in enhancing predictive modeling in healthcare, offering a new direction for future work in medical informatics.\n"
      ],
      "metadata": {
        "id": "AWre3DFh3tYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GITHUB REPO: https://github.com/tamburelloai/DL4H_Project"
      ],
      "metadata": {
        "id": "di3WauP8xm_6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Scope of reproducibility**\n",
        "\n",
        "**Hypothesis 1: Fusion Models Outperform Single-Data-Type Models**\n",
        "* Null Hypothesis (H0):\n",
        "  * The performance of predictive models that combine structured and unstructured data (fusion models) is equal to or worse than models that use either structured data or unstructured data alone.\n",
        "* Alternative Hypothesis (H1):\n",
        "  * Fusion models that combine structured and unstructured data significantly outperform models that utilize either data type alone in terms of predictive accuracy for in-hospital mortality, 30-day hospital readmission, and long length of stay predictions.\n",
        "\n",
        "\n",
        "\n",
        "**Hypothesis 2: Deep Learning Techniques Are Effective for Data Fusion**\n",
        "* Null Hypothesis (H0):\n",
        "  * Deep learning techniques (CNNs and LSTMs) do not offer any significant advantage over traditional machine learning methods when fusing structured and unstructured data for predictive modeling.\n",
        "* Alternative Hypothesis (H1):\n",
        "  * Deep learning techniques, specifically CNNs and LSTMs, are more effective than traditional machine learning methods in fusing structured and unstructured data, leading to better patient representation and improved predictive model performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U6vMV0d732x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Methodology**\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "bLicE70I4aDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## **METHODOLOGY (DATA)**\n"
      ],
      "metadata": {
        "id": "v1KUAp0O4doD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INSTALL PACKAGES**"
      ],
      "metadata": {
        "id": "NoUhzcc5FcRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install tqdm\n",
        "# !pip install scikit-learn\n",
        "# !pip install gensim\n",
        "# !pip install nltk\n",
        "# !pip install --upgrade google-cloud-bigquery\n",
        "# !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2"
      ],
      "metadata": {
        "id": "Dl0k9S0mFdJ5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "c5Vdpl9lEs3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import *\n",
        "import numpy as np\n",
        "import sys\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import argparse\n",
        "from google.colab import auth\n",
        "import gzip\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import random\n",
        "import shutil\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "W7D5l-L-mE2w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENVIRONMENT SETUP**"
      ],
      "metadata": {
        "id": "y8BBTtUGJG9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "regex_punctuation = re.compile('[\\',\\.\\-/\\n]')\n",
        "regex_alphanum = re.compile('[^a-zA-Z0-9 ]')\n",
        "regex_num = re.compile('\\d[\\d ]+')\n",
        "regex_spaces = re.compile('\\s+')\n",
        "#drive.mount('/content/drive')\n",
        "#auth.authenticate_user()\n",
        "#project_id = 'dl4h-418121'\n",
        "#client = bigquery.Client(project=project_id)\n",
        "#dataset_id = f\"{client.project}.my_dataset\"\n",
        "#dataset = bigquery.Dataset(dataset_id)\n",
        "##dataset.location = \"US\"  # Choose the appropriate location\n",
        "#dataset.description = \"Dataset for storing my BigQuery views and tables.\"\n",
        "#client.create_dataset(dataset, timeout=30)  # API request\n",
        "#print(f\"Dataset {dataset_id} created.\")"
      ],
      "metadata": {
        "id": "Bxke4y7sI3w6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65496843-e470-4daf-be47-dfe3a7ebfd4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_links = {\n",
        "  \"ADMISSIONS\": 'https://drive.google.com/uc?id=1ol8txS_oEBOFOmv_T2SyLMTOXptKA-Dp',\n",
        "  \"CALLOUT\": 'https://drive.google.com/uc?id=1f78YVaf818xI_htBEv7GijiP3rNRE97d',\n",
        "  \"CAREGIVERS\": 'https://drive.google.com/uc?id=1LbHVLg1e5MRAI9JsxekRyLjEK_PT_qKw',\n",
        "  \"D_CPT\": 'https://drive.google.com/uc?id=1ckNYCpkgkjApMPN4URIkiBk6NkDUrY3c',\n",
        "  \"D_ITEMS\": 'https://drive.google.com/uc?id=1FUXSkY1CvL8LIPO3XXh-A1INj9nzz6fd',\n",
        "  \"D_LABITEMS\": 'https://drive.google.com/uc?id=1igSZqQPcZzzXqgdOv-hUdJLLFDZcUzAJ',\n",
        "  \"INPUTEVENTS_CV\": 'https://drive.google.com/uc?id=1mb0ml88R881dRaX4klJJOv8am5EhePXw',\n",
        "  \"LABEVENTS\": 'https://drive.google.com/uc?id=1a2JBAMi6RR13egizHtc99xVQvjeYvnqa',\n",
        "  \"MICROBIOLOGYEVENTS\": 'https://drive.google.com/uc?id=1sMU3ldNY6udF31-BZGvZ87fgLCfIsQqg',\n",
        "  \"NOTEEVENTS\": 'https://drive.google.com/uc?id=13y9-jwfdL40GbPWfaJBpSHz2VTlxElRU',\n",
        "  \"PROCEDUREEVENTS\": 'https://drive.google.com/uc?id=1AYAYIM-z_JbrJxk3RgVaU0mZy2BpRlSg',\n",
        "  \"PROCEDURES_ICD\": 'https://drive.google.com/uc?id=10ofQEK_ziA9IFWNPkYem0xGVeUX-r4Sp',\n",
        "  \"PATIENTS\": 'https://drive.google.com/uc?export=download&id=1u2fsivNmC5OU8M_qkPBc0ViGSYFK0-7Y',\n",
        "  \"adm_demo_data\" : \"https://drive.google.com/uc?export=download&id=1YOzMKF_nqDgJjnnvujoWqFGv7CNRwXuK\",\n",
        "  \"icd_demo_data\": \"https://drive.google.com/uc?export=download&id=1-1Q6bsOpJ9NgcjI5ejQIHkKsxXapC77h\",\n",
        "\n",
        "  }\n"
      ],
      "metadata": {
        "id": "6y_bwWFVWhXw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UTILITIES**"
      ],
      "metadata": {
        "id": "R7e7bpsFEomU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Utility Functions"
      ],
      "metadata": {
        "id": "sMZlqSk6IT54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bin_age(age):\n",
        "    if age < 25:\n",
        "        return '18-25'\n",
        "    elif age < 45:\n",
        "        return '25-45'\n",
        "    elif age < 65:\n",
        "        return '45-65'\n",
        "    elif age < 89:\n",
        "        return '65-89'\n",
        "    else:\n",
        "        return '89+'\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # remove phi tags\n",
        "    tags = re.findall('\\[\\*\\*.*?\\*\\*\\]', text)\n",
        "    for tag in set(tags):\n",
        "        text = text.replace(tag, ' ')\n",
        "\n",
        "    text = re.sub(regex_punctuation, ' ', text)\n",
        "    text = re.sub(regex_alphanum, '', text)\n",
        "    text = re.sub(regex_num, ' 0 ', text)\n",
        "    text = re.sub(regex_spaces, ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def text2words(text):\n",
        "    words = text.split()\n",
        "    words = [w for w in words if not w in stops]\n",
        "    return words\n",
        "\n",
        "\n",
        "def convert_icd_group(icd):\n",
        "    icd = str(icd)\n",
        "    if icd.startswith('V'):\n",
        "        return 19\n",
        "    if icd.startswith('E'):\n",
        "        return 20\n",
        "    icd = int(icd[:3])\n",
        "    if icd <= 139:\n",
        "        return 1\n",
        "    elif icd <= 239:\n",
        "        return 2\n",
        "    elif icd <= 279:\n",
        "        return 3\n",
        "    elif icd <= 289:\n",
        "        return 4\n",
        "    elif icd <= 319:\n",
        "        return 5\n",
        "    elif icd <= 389:\n",
        "        return 6\n",
        "    elif icd <= 459:\n",
        "        return 7\n",
        "    elif icd <= 519:\n",
        "        return 8\n",
        "    elif icd <= 579:\n",
        "        return 9\n",
        "    elif icd < 629:\n",
        "        return 10\n",
        "    elif icd <= 679:\n",
        "        return 11\n",
        "    elif icd <= 709:\n",
        "        return 12\n",
        "    elif icd <= 739:\n",
        "        return 13\n",
        "    elif icd <= 759:\n",
        "        return 14\n",
        "    elif icd <= 779:\n",
        "        return np.nan\n",
        "    elif icd <= 789:\n",
        "        return 15\n",
        "    elif icd <= 796:\n",
        "        return 16\n",
        "    elif icd <= 799:\n",
        "        return 17\n",
        "    else:\n",
        "        return 18\n",
        "\n",
        "\n",
        "def cal_metric(y_true, probs):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, probs)\n",
        "    optimal_idx = np.argmax(np.sqrt(tpr * (1-fpr)))\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    preds = (probs > optimal_threshold).astype(int)\n",
        "    auc = metrics.roc_auc_score(y_true, probs)\n",
        "    auprc = metrics.average_precision_score(y_true, probs)\n",
        "    f1 = metrics.f1_score(y_true, preds)\n",
        "    return f1, auc, auprc\n",
        "\n",
        "\n",
        "def save_model(all_dict, name='best_model.pth'):\n",
        "    model_dir = all_dict['args'].model_dir\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.mkdir(model_dir)\n",
        "    model_path = os.path.join(model_dir, name)\n",
        "    torch.save(all_dict, model_path)\n",
        "\n",
        "\n",
        "def load_model(model_dict, name='best_model.pth'):\n",
        "    model = model_dict['model']\n",
        "    model_dir = model_dict['args'].model_dir\n",
        "    model_path = os.path.join(model_dir, name)\n",
        "    if os.path.exists(model_path):\n",
        "        all_dict = torch.load(model_path)\n",
        "        model.load_state_dict(all_dict['state_dict'])\n",
        "        return model, all_dict['best_metric'], all_dict['epoch']\n",
        "    else:\n",
        "        return model, 0, 1\n",
        "\n",
        "\n",
        "def get_ids(split_json):\n",
        "    splits = list(range(10))\n",
        "    adm_ids = json.load(open(split_json))\n",
        "    train_ids = np.hstack([adm_ids[t] for t in splits[:7]])\n",
        "    val_ids = np.hstack([adm_ids[t] for t in splits[7:8]])\n",
        "    test_ids = np.hstack([adm_ids[t] for t in splits[8:]])\n",
        "    train_ids = [adm_id[-10:-4] for adm_id in train_ids]\n",
        "    val_ids = [adm_id[-10:-4] for adm_id in val_ids]\n",
        "    test_ids = [adm_id[-10:-4] for adm_id in test_ids]\n",
        "    return train_ids, val_ids, test_ids\n",
        "\n",
        "\n",
        "def get_ids2(split_json, seed):\n",
        "    splits = list(range(10))\n",
        "    random.Random(seed).shuffle(splits)\n",
        "    adm_ids = json.load(open(split_json))\n",
        "    train_ids = np.hstack([adm_ids[t] for t in splits[:7]])\n",
        "    val_ids = np.hstack([adm_ids[t] for t in splits[7:8]])\n",
        "    test_ids = np.hstack([adm_ids[t] for t in splits[8:]])\n",
        "    train_ids = [adm_id[-10:-4] for adm_id in train_ids]\n",
        "    val_ids = [adm_id[-10:-4] for adm_id in val_ids]\n",
        "    test_ids = [adm_id[-10:-4] for adm_id in test_ids]\n",
        "    return train_ids, val_ids, test_ids\n",
        "\n",
        "\n",
        "def balance_samples(df, times, task):\n",
        "    df_pos = df[df[task] == 1]\n",
        "    df_neg = df[df[task] == 0]\n",
        "    df_neg = df_neg.sample(n=times * len(df_pos), random_state=42)\n",
        "    df = pd.concat([df_pos, df_neg]).sort_values('hadm_id')\n",
        "    return df\n",
        "\n",
        "\n",
        "def mkdir(d):\n",
        "    path = [x for x in d.split('/') if len(x)]\n",
        "    for i in range(len(path)):\n",
        "        d = '/'.join(path[:i+1])\n",
        "        if not os.path.exists(d):\n",
        "            os.mkdir(d)\n",
        "\n",
        "\n",
        "def csv_split(line, sc=','):\n",
        "    res = []\n",
        "    inside = 0\n",
        "    s = ''\n",
        "    for c in line:\n",
        "        if inside == 0 and c == sc:\n",
        "            res.append(s)\n",
        "            s = ''\n",
        "        else:\n",
        "            if c == '\"':\n",
        "                inside = 1 - inside\n",
        "            s = s + c\n",
        "    res.append(s)\n",
        "    return res\n",
        "\n",
        "\n",
        "def unzip(zipped_file, csv_file):\n",
        "  try:\n",
        "    # Open the .gz file in binary read mode ('rb') and the output file in binary write mode ('wb')\n",
        "    with gzip.open(zipped_file, 'rb') as gz_file:\n",
        "      with open(csv_file, 'wb') as output_file:\n",
        "        # Copy the contents of the .gz file to the output file, decompressing it in the process\n",
        "        shutil.copyfileobj(gz_file, output_file)\n",
        "  except gzip.BadGzipFile:\n",
        "      print(f\"The file {zipped_file} is not a valid gzip file or is corrupted.\")"
      ],
      "metadata": {
        "id": "mzWWmGScEWbD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Inference Utility Functions"
      ],
      "metadata": {
        "id": "sFrSCgHbmNbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _cuda(tensor, is_tensor=True):\n",
        "    if args.gpu:\n",
        "        if is_tensor:\n",
        "            return tensor.cuda()\n",
        "        else:\n",
        "            return tensor.cuda()\n",
        "    else:\n",
        "        return tensor\n",
        "\n",
        "def get_lr(epoch):\n",
        "    lr = args.lr\n",
        "    return lr\n",
        "\n",
        "    if epoch <= args.epochs * 0.5:\n",
        "        lr = args.lr\n",
        "    elif epoch <= args.epochs * 0.75:\n",
        "        lr = 0.1 * args.lr\n",
        "    elif epoch <= args.epochs * 0.9:\n",
        "        lr = 0.01 * args.lr\n",
        "    else:\n",
        "        lr = 0.001 * args.lr\n",
        "    return lr\n",
        "\n",
        "def index_value(data):\n",
        "    '''\n",
        "    map data to index and value\n",
        "    '''\n",
        "    if args.use_ve == 0:\n",
        "        data = Variable(_cuda(data)) # [bs, 250]\n",
        "        return data\n",
        "    data = data.numpy()\n",
        "    index = data / (args.split_num + 1)\n",
        "    value = data % (args.split_num + 1)\n",
        "    index = Variable(_cuda(torch.from_numpy(index.astype(np.int64))))\n",
        "    value = Variable(_cuda(torch.from_numpy(value.astype(np.int64))))\n",
        "    return [index, value]\n",
        "\n",
        "def train_eval(data_loader, net, loss, epoch, optimizer, best_metric, phase='train'):\n",
        "    print(phase)\n",
        "    lr = get_lr(epoch)\n",
        "    if phase == 'train':\n",
        "        net.train()\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "    else:\n",
        "        net.eval()\n",
        "\n",
        "    loss_list, pred_list, label_list, = [], [], []\n",
        "    for b, data_list in enumerate(tqdm(data_loader)):\n",
        "        data, dtime, demo, content, label, files = data_list\n",
        "        if args.value_embedding == 'no':\n",
        "            data = Variable(_cuda(data))\n",
        "        else:\n",
        "            data = index_value(data)\n",
        "\n",
        "\n",
        "        dtime = Variable(_cuda(dtime))\n",
        "        demo = Variable(_cuda(demo))\n",
        "        content = Variable(_cuda(content))\n",
        "        label = Variable(_cuda(label))\n",
        "        output = net(data, dtime, demo, content) # [bs, 1]\n",
        "        # output = net(data, dtime, demo) # [bs, 1]\n",
        "\n",
        "\n",
        "\n",
        "        loss_output = loss(output, label)\n",
        "        pred_list.append(output.data.cpu().numpy())\n",
        "        loss_list.append(loss_output[0].data.cpu().numpy())\n",
        "        label_list.append(label.data.cpu().numpy())\n",
        "\n",
        "        if phase == 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss_output[0].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    pred = np.concatenate(pred_list, 0)\n",
        "    label = np.concatenate(label_list, 0)\n",
        "    if len(pred.shape) == 1:\n",
        "        metric = function.compute_auc(label, pred)\n",
        "    else:\n",
        "        metrics = []\n",
        "        auc_metrics = []\n",
        "        for i_shape in range(pred.shape[1]):\n",
        "            metric0 = cal_metric(label[:, i_shape], pred[:, i_shape])\n",
        "            auc_metric = function.compute_auc(label[:, i_shape], pred[:, i_shape])\n",
        "            # print('........AUC_{:d}: {:3.4f}, AUPR_{:d}: {:3.4f}'.format(i_shape, auc, i_shape, aupr))\n",
        "            print(i_shape + 1, metric0)\n",
        "            metrics.append(metric0)\n",
        "            auc_metrics.append(auc_metric)\n",
        "        print('Avg', np.mean(metrics, axis=0).tolist())\n",
        "        metric = np.mean(auc_metrics)\n",
        "    avg_loss = np.mean(loss_list)\n",
        "\n",
        "    print('\\n{:s} Epoch {:d} (lr {:3.6f})'.format(phase, epoch, lr))\n",
        "    print('loss: {:3.4f} \\t'.format(avg_loss))\n",
        "    if phase == 'valid' and best_metric[0] < metric:\n",
        "        best_metric = [metric, epoch]\n",
        "        function.save_model({'args': args, 'model': net, 'epoch':epoch, 'best_metric': best_metric})\n",
        "    if phase != 'train':\n",
        "        print('\\t\\t\\t\\t best epoch: {:d}     best AUC: {:3.4f} \\t'.format(best_metric[1], best_metric[0]))\n",
        "    return best_metric"
      ],
      "metadata": {
        "id": "VmQdHaZfmRDn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIGQUERY VIEW AND TABLE GENERATION**"
      ],
      "metadata": {
        "id": "YhxrhFzvEEUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigQuery Queries"
      ],
      "metadata": {
        "id": "nfFh-9VVKcfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADM_DETAILS Query\n"
      ],
      "metadata": {
        "id": "0tvMZ-w8Kvic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adm_details_query = \"\"\"\n",
        "SELECT\n",
        "  p.subject_id,\n",
        "  p.gender,\n",
        "  p.dob,\n",
        "  p.dod,\n",
        "  adm.hadm_id,\n",
        "  adm.admittime,\n",
        "  adm.dischtime,\n",
        "  adm.admission_type,\n",
        "  adm.insurance,\n",
        "  adm.marital_status,\n",
        "  adm.ethnicity,\n",
        "  adm.hospital_expire_flag,\n",
        "  adm.has_chartevents_data\n",
        "FROM\n",
        "  `physionet-data.mimiciii_clinical.admissions` adm\n",
        "JOIN\n",
        "  `physionet-data.mimiciii_clinical.patients` p\n",
        "ON\n",
        "  adm.subject_id = p.subject_id\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yA8aYXhiIjpm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIVOTED_LABS Query\n"
      ],
      "metadata": {
        "id": "-xRqmRDILHR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivoted_labs_query = \"\"\"\n",
        "WITH icu_stays AS (\n",
        "  SELECT\n",
        "    subject_id, icustay_id, intime, outtime,\n",
        "    LAG(outtime) OVER (PARTITION BY subject_id ORDER BY intime) AS outtime_lag,\n",
        "    LEAD(intime) OVER (PARTITION BY subject_id ORDER BY intime) AS intime_lead\n",
        "  FROM `physionet-data.mimiciii_clinical.icustays`\n",
        "),\n",
        "icu_stays_adjusted AS (\n",
        "  SELECT\n",
        "    subject_id, icustay_id,\n",
        "    CASE\n",
        "      WHEN outtime_lag IS NOT NULL AND TIMESTAMP_DIFF(intime, outtime_lag, HOUR) < 24\n",
        "      THEN TIMESTAMP_SUB(intime, INTERVAL DIV(TIMESTAMP_DIFF(intime, outtime_lag, MINUTE), 2) MINUTE)\n",
        "      ELSE TIMESTAMP_SUB(intime, INTERVAL 12 HOUR)\n",
        "    END AS data_start,\n",
        "    CASE\n",
        "      WHEN intime_lead IS NOT NULL AND TIMESTAMP_DIFF(intime_lead, outtime, HOUR) < 24\n",
        "      THEN TIMESTAMP_ADD(outtime, INTERVAL DIV(TIMESTAMP_DIFF(intime_lead, outtime, MINUTE), 2) MINUTE)\n",
        "      ELSE TIMESTAMP_ADD(outtime, INTERVAL 12 HOUR)\n",
        "    END AS data_end\n",
        "  FROM icu_stays\n",
        "),\n",
        "admissions_adjusted AS (\n",
        "  SELECT\n",
        "    subject_id, hadm_id, admittime, dischtime,\n",
        "    LAG(dischtime) OVER (PARTITION BY subject_id ORDER BY admittime) AS dischtime_lag,\n",
        "    LEAD(admittime) OVER (PARTITION BY subject_id ORDER BY admittime) AS admittime_lead\n",
        "  FROM `physionet-data.mimiciii_clinical.admissions`\n",
        "),\n",
        "admissions_boundaries AS (\n",
        "  SELECT\n",
        "    subject_id, hadm_id,\n",
        "    CASE\n",
        "      WHEN dischtime_lag IS NOT NULL AND TIMESTAMP_DIFF(admittime, dischtime_lag, HOUR) < 24\n",
        "      THEN TIMESTAMP_SUB(admittime, INTERVAL DIV(TIMESTAMP_DIFF(admittime, dischtime_lag, MINUTE), 2) MINUTE)\n",
        "      ELSE TIMESTAMP_SUB(admittime, INTERVAL 12 HOUR)\n",
        "    END AS data_start,\n",
        "    CASE\n",
        "      WHEN admittime_lead IS NOT NULL AND TIMESTAMP_DIFF(admittime_lead, dischtime, HOUR) < 24\n",
        "      THEN TIMESTAMP_ADD(dischtime, INTERVAL DIV(TIMESTAMP_DIFF(admittime_lead, dischtime, MINUTE), 2) MINUTE)\n",
        "      ELSE TIMESTAMP_ADD(dischtime, INTERVAL 12 HOUR)\n",
        "    END AS data_end\n",
        "  FROM admissions_adjusted\n",
        "),\n",
        "lab_events_filtered AS (\n",
        "  SELECT\n",
        "    subject_id, charttime,\n",
        "    CASE\n",
        "      WHEN itemid = 50868 THEN 'ANION GAP'\n",
        "      -- Add other itemid mappings here\n",
        "    END AS label,\n",
        "    CASE\n",
        "      WHEN itemid = 50862 AND valuenum > 10 THEN NULL -- Example condition\n",
        "      ELSE valuenum\n",
        "    END AS valuenum\n",
        "  FROM `physionet-data.mimiciii_clinical.labevents`\n",
        "  WHERE itemid IN (50868, 50862) -- Add other itemids here\n",
        "    AND valuenum IS NOT NULL AND valuenum > 0\n",
        "),\n",
        "lab_events_avg AS (\n",
        "  SELECT\n",
        "    subject_id, charttime,\n",
        "    AVG(CASE WHEN label = 'ANION GAP' THEN valuenum ELSE NULL END) AS anion_gap,\n",
        "    -- Add other lab result averages here\n",
        "  FROM lab_events_filtered\n",
        "  GROUP BY subject_id, charttime\n",
        ")\n",
        "SELECT\n",
        "  i.icustay_id, a.hadm_id, l.*\n",
        "FROM lab_events_avg l\n",
        "LEFT JOIN admissions_boundaries a ON l.subject_id = a.subject_id\n",
        "  AND l.charttime >= a.data_start\n",
        "  AND l.charttime < a.data_end\n",
        "LEFT JOIN icu_stays_adjusted i ON l.subject_id = i.subject_id\n",
        "  AND l.charttime >= i.data_start\n",
        "  AND l.charttime < i.data_end\n",
        "ORDER BY l.subject_id, l.charttime;\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2a-FtliNLG4c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### PIVOTED_VITALS Query\n"
      ],
      "metadata": {
        "id": "8z9T95hPLIJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivoted_vitals_query = \"\"\"\n",
        "WITH ce AS (\n",
        "  SELECT\n",
        "    ce.icustay_id,\n",
        "    ce.charttime,\n",
        "    MAX(CASE WHEN itemid IN (211,220045) AND valuenum > 0 AND valuenum < 300 THEN valuenum ELSE NULL END) AS HeartRate,\n",
        "    MAX(CASE WHEN itemid IN (51,442,455,6701,220179,220050) AND valuenum > 0 AND valuenum < 400 THEN valuenum ELSE NULL END) AS SysBP,\n",
        "    MAX(CASE WHEN itemid IN (8368,8440,8441,8555,220180,220051) AND valuenum > 0 AND valuenum < 300 THEN valuenum ELSE NULL END) AS DiasBP,\n",
        "    MAX(CASE WHEN itemid IN (456,52,6702,443,220052,220181,225312) AND valuenum > 0 AND valuenum < 300 THEN valuenum ELSE NULL END) AS MeanBP,\n",
        "    MAX(CASE WHEN itemid IN (615,618,220210,224690) AND valuenum > 0 AND valuenum < 70 THEN valuenum ELSE NULL END) AS RespRate,\n",
        "    MAX(CASE\n",
        "        WHEN itemid IN (223761,678) AND valuenum > 70 AND valuenum < 120 THEN (valuenum-32)/1.8\n",
        "        WHEN itemid IN (223762,676) AND valuenum > 10 AND valuenum < 50 THEN valuenum\n",
        "        ELSE NULL\n",
        "      END) AS TempC,\n",
        "    MAX(CASE WHEN itemid IN (646,220277) AND valuenum > 0 AND valuenum <= 100 THEN valuenum ELSE NULL END) AS SpO2,\n",
        "    MAX(CASE WHEN itemid IN (807,811,1529,3745,3744,225664,220621,226537) AND valuenum > 0 THEN valuenum ELSE NULL END) AS Glucose\n",
        "  FROM\n",
        "    `physionet-data.mimiciii_clinical.chartevents` ce\n",
        "  WHERE\n",
        "    (ce.error IS NULL OR ce.error != 1)\n",
        "    AND ce.itemid IN (211,220045,51,442,455,6701,220179,220050,8368,8440,8441,8555,220180,220051,456,52,6702,443,220052,220181,225312,615,618,220210,224690,646,220277,223761,678,223762,676,807,811,1529,3745,3744,225664,220621,226537)\n",
        "  GROUP BY\n",
        "    ce.icustay_id, ce.charttime\n",
        ")\n",
        "SELECT\n",
        "  icustays.hadm_id,\n",
        "  ce.charttime,\n",
        "  AVG(HeartRate) AS HeartRate,\n",
        "  AVG(SysBP) AS SysBP,\n",
        "  AVG(DiasBP) AS DiasBP,\n",
        "  AVG(MeanBP) AS MeanBP,\n",
        "  AVG(RespRate) AS RespRate,\n",
        "  AVG(TempC) AS TempC,\n",
        "  AVG(SpO2) AS SpO2,\n",
        "  AVG(Glucose) AS Glucose\n",
        "FROM\n",
        "  `physionet-data.mimiciii_clinical.icustays` icustays\n",
        "LEFT JOIN ce ON ce.icustay_id = icustays.icustay_id\n",
        "GROUP BY\n",
        "  icustays.hadm_id, ce.charttime\n",
        "ORDER BY\n",
        "  icustays.hadm_id, ce.charttime;\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kPMiMy4OLMZ0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Queries and Generate Dataframes"
      ],
      "metadata": {
        "id": "2_2hqcQNL7yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actually_query_private_dataset = False\n",
        "\n",
        "if actually_query_private_dataset:\n",
        "  adm_details_df = client.query(adm_details_query).result().to_dataframe()     # Execute adm_details query and convert to a DataFrame\n",
        "  pivoted_labs_df = client.query(pivoted_labs_query).result().to_dataframe()    # Execute the pivoted_labs query and convert to a DataFrame\n",
        "  pivoted_vitals_df = client.query(pivoted_vitals_query).result().to_dataframe()  # Execute the pivoted_vitals query and convert to a DataFrame\n",
        "else:\n",
        "  pass"
      ],
      "metadata": {
        "id": "9eugkgYbMBNw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Diagnosis Table from BigQuery"
      ],
      "metadata": {
        "id": "XCc9RIKnNRtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagnosis_table_query = \"\"\"\n",
        "SELECT *\n",
        "FROM `physionet-data.mimiciii_clinical.diagnoses_icd`\n",
        "\"\"\"\n",
        "\n",
        "if actually_query_private_dataset:\n",
        "  diagnoses_df = client.query(diagnosis_table_query).result().to_dataframe()\n",
        "else:\n",
        "  print(f\"Querying BigQuery for diagnosis table...[DONE]\")"
      ],
      "metadata": {
        "id": "iFNdi8Q5NRtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39db979a-d47e-4503-9b7f-a3744268f247"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying BigQuery for diagnosis table...[DONE]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Note Events Table from BigQuery"
      ],
      "metadata": {
        "id": "0KGMQCfNNSEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM `physionet-data.mimiciii_notes.noteevents`\n",
        "\"\"\"\n",
        "\n",
        "if actually_query_private_dataset:\n",
        "  note_events_df = client.query(query).result().to_dataframe()\n",
        "else:\n",
        "  print(f\"Querying BigQuery for note events table...[DONE]\")"
      ],
      "metadata": {
        "id": "Q49UdIBQNSEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41cc1f15-e441-4853-9608-7c367be77c99"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying BigQuery for note events table...[DONE]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVE ALL TABLES (DATAFRAMES) TO GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "tRkI7XjCO4HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if actually_query_private_dataset:\n",
        "  adm_details_df.to_csv('/content/drive/MyDrive/mimic-iii_processed_data/adm_details.csv', index=False)\n",
        "  pivoted_labs_df.to_csv('/content/drive/MyDrive/mimic-iii_processed_data/pivoted_lab.csv', index=False)\n",
        "  pivoted_vitals_df.to_csv('/content/drive/MyDrive/mimic-iii_processed_data/pivoted_vitals.csv', index=False)\n",
        "  diagnoses_df.to_csv('/content/drive/MyDrive/mimic-iii_processed_data/diagnoses.csv', index=False)\n",
        "  note_events_df.to_csv('/content/drive/MyDrive/mimic-iii_processed_data/noteevents.csv', index=False)\n",
        "else:\n",
        "  print(\"Saving all tables to .CSV files for later analysis and use...[DONE]\")"
      ],
      "metadata": {
        "id": "PVS6SHmVO-dK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba84191-602f-4fba-8358-c6ebcb993ac9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all tables to .CSV files for later analysis and use...[DONE]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESSING**"
      ],
      "metadata": {
        "id": "jKki5XFIOb18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 1: Preliminary Table Setup\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "piVQNh0xhNyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if actually_query_private_dataset:\n",
        "  df_adm = pd.read_csv('/content/drive/MyDrive/mimic-iii_processed_data/adm_details.csv', parse_dates=['dob', 'dod', 'admittime', 'dischtime'])\n",
        "  df_icd = pd.read_csv('/content/drive/MyDrive/mimic-iii_processed_data/diagnoses.csv')[['HADM_ID', 'ICD9_CODE']].dropna()\n",
        "\n",
        "else:\n",
        "  df_adm = pd.read_csv(data_links['adm_demo_data'], parse_dates=['dob', 'dod', 'admittime', 'dischtime'])\n",
        "  df_icd = pd.read_csv(data_links['icd_demo_data'])[['HADM_ID', 'ICD9_CODE']].dropna()\n",
        "\n",
        "\n",
        "\n",
        "df_adm['age'] = df_adm['admittime'].dt.year - df_adm['dob'].dt.year\n",
        "birthday_not_yet = (df_adm['admittime'].dt.month < df_adm['dob'].dt.month) | ((df_adm['admittime'].dt.month == df_adm['dob'].dt.month) & (df_adm['admittime'].dt.day < df_adm['dob'].dt.day))\n",
        "df_adm['age'] -= birthday_not_yet.astype(int)\n",
        "df_adm['age'] = df_adm['age'].astype(int)\n",
        "df_adm['los'] = (df_adm['dischtime'] - df_adm['admittime']) / np.timedelta64(1, 'D')\n",
        "df_adm = df_adm[df_adm['age'] >= 18]  # keep adults\n",
        "df_adm['age'] = df_adm['age'].apply(bin_age)\n",
        "print('After removing non-adults:', len(df_adm))\n",
        "df_adm = df_adm[df_adm['los'] >= 1]  # keep more than 1 day\n",
        "print('After removing less than 1 day:', len(df_adm))\n",
        "df_adm = df_adm.sort_values(['subject_id', 'admittime']).reset_index(drop=True)\n",
        "print('Processing patients demographics...')\n",
        "df_adm['marital_status'] = df_adm['marital_status'].fillna('Unknown')\n",
        "df_static = df_adm[['hadm_id', 'age', 'gender', 'admission_type', 'insurance',\n",
        "        'marital_status', 'ethnicity']]\n",
        "df_static.to_csv('static_demo.csv', index=None)\n",
        "\n",
        "print('Collecting labels...')\n",
        "df_icd.columns = map(str.lower, df_icd.columns)\n",
        "df_icd['icd9_code'] = df_icd['icd9_code'].apply(convert_icd_group)\n",
        "df_icd = df_icd.dropna().drop_duplicates().sort_values(['hadm_id', 'icd9_code'])\n",
        "for x in range(20):\n",
        "    x += 1\n",
        "    df_icd[f'{x}'] = (df_icd['icd9_code'] == x).astype(int)\n",
        "df_icd = df_icd.groupby('hadm_id').sum()\n",
        "df_icd = df_icd[df_icd.columns[1:]].reset_index()\n",
        "df_icd = df_icd[df_icd.hadm_id.isin(df_adm.hadm_id)]\n",
        "\n",
        "df_readmit = df_adm.copy()\n",
        "df_readmit['next_admittime'] = df_readmit.groupby(\n",
        "    'subject_id')['admittime'].shift(-1)\n",
        "df_readmit['next_admission_type'] = df_readmit.groupby(\n",
        "    'subject_id')['admission_type'].shift(-1)\n",
        "elective_rows = df_readmit['next_admission_type'] == 'ELECTIVE'\n",
        "df_readmit.loc[elective_rows, 'next_admittime'] = pd.NaT\n",
        "df_readmit.loc[elective_rows, 'next_admission_type'] = np.NaN\n",
        "df_readmit[['next_admittime', 'next_admission_type']] = df_readmit.groupby(\n",
        "    ['subject_id'])[['next_admittime', 'next_admission_type']].fillna(method='bfill')\n",
        "df_readmit['days_next_admit'] = (\n",
        "    df_readmit['next_admittime'] - df_readmit['dischtime']).dt.total_seconds() / (24 * 60 * 60)\n",
        "df_readmit['readmit'] = (\n",
        "    df_readmit['days_next_admit'] < 30).astype('int')\n",
        "\n",
        "print('Done.')\n",
        "df_labels = df_adm[['hadm_id', 'los']]\n",
        "df_labels['mortality'] = df_adm['hospital_expire_flag']\n",
        "df_labels['readmit'] = df_readmit['readmit']\n",
        "\n",
        "df_labels[['hadm_id', 'los']].to_csv('los.csv', index=None)\n",
        "df_labels[['hadm_id', 'mortality']].to_csv('mortality.csv', index=None)\n",
        "df_labels[['hadm_id', 'readmit']].to_csv('readmit.csv', index=None)\n",
        "df_icd.to_csv('labels_icd.csv', index=None)\n",
        "df_static.to_csv('labels_static.csv', index=None)\n",
        "\n",
        "\n",
        "df_adm.to_csv('adm_details.csv', index=None)\n",
        "df_icd.to_csv('diagnoses.csv', index=None)\n"
      ],
      "metadata": {
        "id": "aquNdLo7hjsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fa75ab-7dd5-4b62-a514-dd17c3a33f5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing non-adults: 88\n",
            "After removing less than 1 day: 84\n",
            "Processing patients demographics...\n",
            "Collecting labels...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 2: Get Signals\n",
        "\n"
      ],
      "metadata": {
        "id": "W3gl8i9ahkxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_demo_signals():\n",
        "  print(\"Loading demo signals later from drive link\")\n",
        "\n",
        "def get_signals(start_hr, end_hr):\n",
        "  root = \"/content/drive/MyDrive\"\n",
        "  df_adm = pd.read_csv(f'adm_details.csv', parse_dates=['admittime'])\n",
        "  adm_ids = df_adm.hadm_id.tolist()\n",
        "  for signal in ['vitals', 'lab']:\n",
        "    df = pd.read_csv(f'pivoted_{signal}.csv', parse_dates=['charttime'])\n",
        "    df = df.merge(df_adm[['hadm_id', 'admittime']], on='hadm_id')\n",
        "    df = df[df.hadm_id.isin(adm_ids)]\n",
        "    df['hr'] = (df.charttime - df.admittime) / np.timedelta64(1, 'h')\n",
        "    df = df[(df.hr <= end_hr) & (df.hr >= start_hr)]\n",
        "    df = df.set_index('hadm_id').groupby('hadm_id').resample('H', on='charttime').mean().reset_index()\n",
        "    df.to_csv(f'{signal}.csv', index=None)\n",
        "  df = pd.read_csv(f'vitals.csv', parse_dates=['charttime'])\n",
        "  df.columns = map(str.lower, df.columns)\n",
        "  df = df[['hadm_id', 'charttime', 'heartrate', 'sysbp', 'diasbp', 'meanbp', 'resprate', 'tempc', 'spo2']]\n",
        "  print(df.shape, df.columns)\n",
        "  df_lab = pd.read_csv(f'lab.csv',parse_dates=['charttime'])\n",
        "  df = df.merge(df_lab, on=['hadm_id', 'charttime'], how='outer')\n",
        "  df = df.merge(df_adm[['hadm_id', 'admittime']], on='hadm_id')\n",
        "  df['charttime'] = ((df.charttime - df.admittime) / np.timedelta64(1, 'h'))\n",
        "  df['charttime'] = df['charttime'].apply(np.ceil) + 1\n",
        "  df = df[(df.charttime <= end_hr) & (df.charttime >= start_hr)]\n",
        "  df = df.sort_values(['hadm_id', 'charttime'])\n",
        "  df['charttime'] = df['charttime'].map(lambda x: int(x))\n",
        "  df = df.drop(['admittime', 'hr'], axis=1)\n",
        "  na_thres = 3\n",
        "  df = df.dropna(thresh=na_thres)\n",
        "  df.to_csv(f'features.csv', index=None)\n",
        "\n",
        "if actually_query_private_dataset:\n",
        "   get_signals(1, 8)\n",
        "else:\n",
        "  load_demo_signals()\n"
      ],
      "metadata": {
        "id": "H51xkyGPhkxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440063d8-56cd-466a-ab3a-d8f39e851afc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading demo signals later from drive link\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 3: Extract Notes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "waVGXM1KhlFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_early(df_notes, early_categories):\n",
        "#     '''Extract first 24 hours notes'''\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     df_early = df_notes[df_notes['category'].isin(early_categories)]\n",
        "#     df_early['hr'] = (df_early['charttime'] - df_early['admittime']) / np.timedelta64(1, 'h')\n",
        "#     df_early = df_early[df_early['hr'] <= 24]\n",
        "#     # df_early = df_early.groupby('hadm_id').head(12).reset_index()\n",
        "#     df_early = df_early.sort_values(['hadm_id', 'hr'])\n",
        "#     df_early['text'] = df_early['text'].apply(clean_text)\n",
        "#     df_early[['hadm_id', 'hr', 'category', 'text']].to_csv(f'{root}/earlynotes.csv', index=None)\n",
        "\n",
        "\n",
        "# def extract_first(df_notes, early_categories):\n",
        "#     '''Extract first 24 notes'''\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     df_early = df_notes[df_notes['category'].isin(early_categories)]\n",
        "#     df_early['hr'] = (df_early['charttime'] - df_early['admittime']) / np.timedelta64(1, 'h')\n",
        "#     df_early = df_early.groupby('hadm_id').head(24).reset_index()\n",
        "#     df_early = df_early.sort_values(['hadm_id', 'hr'])\n",
        "#     df_early['text'] = df_early['text'].apply(clean_text)\n",
        "#     df_early[['hadm_id', 'hr', 'category', 'text']].to_csv(f'{root}/firstnotes.csv', index=None)\n",
        "\n",
        "# args = {\n",
        "#     'firstday': True,\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "# print('Reading data...')\n",
        "# early_categories = ['Nursing', 'Nursing/other', 'Physician ', 'Radiology']\n",
        "# df_notes = pd.read_csv(f'{root}/noteevents.csv')\n",
        "# df_notes['CHARTTIME'] = pd.to_datetime(df_notes['CHARTTIME'])\n",
        "# df_notes.columns = map(str.lower, df_notes.columns)\n",
        "# df_notes = df_notes[df_notes['iserror'].isnull()]\n",
        "# df_notes = df_notes[~df_notes['hadm_id'].isnull()]\n",
        "# df_notes = df_notes[~df_notes['charttime'].isnull()]\n",
        "\n",
        "# df_adm = pd.read_csv(f'{root}/adm_details.csv', parse_dates=['admittime'])\n",
        "# df_notes = df_notes.merge(df_adm, on='hadm_id', how='left')\n",
        "\n",
        "# if args['firstday']:\n",
        "#     print('Extracting first day notes...')\n",
        "#     extract_early(df_notes, early_categories)\n",
        "# else:\n",
        "#     print('Extracting first 24 notes...')\n",
        "#     extract_first(df_notes, early_categories)\n",
        "\n",
        "# extract_first(df_notes, early_categories) # storing first 24 hour notes in case needed later and can avoid earlier processing by starting from having this\n",
        "\n",
        "\n",
        "print(\"Extracting note content --> for demo I will load the demo data later in script\")"
      ],
      "metadata": {
        "id": "m-Z_vOyjhlFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d55436-cd6f-43e5-aa0f-a2b4ebbeacc8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting note content --> for demo I will load the demo data later in script\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 4: Merge IDs\n"
      ],
      "metadata": {
        "id": "TMTlsheVhlSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "# df_static = pd.read_csv(f'{root}/demo.csv')\n",
        "# df_features = pd.read_csv(f'{root}/features.csv')\n",
        "# df_notes = pd.read_csv(f'{root}/earlynotes.csv') # change to firstnotes.csv if doing first 24hr\n",
        "# df_icd = pd.read_csv(f'{root}/labels_icd.csv')\n",
        "# df_notes = df_notes[~df_notes['text'].isnull()]\n",
        "# adm_ids = df_static['hadm_id'].tolist()\n",
        "# adm_ids = np.intersect1d(adm_ids, df_features['hadm_id'].unique().tolist())\n",
        "# adm_ids = np.intersect1d(adm_ids, df_notes['hadm_id'].unique().tolist())\n",
        "# adm_ids = np.intersect1d(adm_ids, df_icd['hadm_id'].unique().tolist())\n",
        "# df_static[df_static['hadm_id'].isin(adm_ids)].to_csv(f'{root}/demo.csv', index=None)\n",
        "# df_features[df_features['hadm_id'].isin(adm_ids)].to_csv(f'{root}/features.csv', index=None)\n",
        "# df_notes[df_notes['hadm_id'].isin(adm_ids)].to_csv(f'{root}/earlynotes.csv', index=None)\n",
        "# for task in ('mortality', 'readmit', 'los'):\n",
        "#     df = pd.read_csv(f'{root}/{task}.csv')\n",
        "#     df[df['hadm_id'].isin(adm_ids)].to_csv(f'{root}/{task}.csv', index=None)\n",
        "# df = pd.read_csv(f'{root}/los.csv')\n",
        "# df['llos'] = (df['los'] > 7).astype(int)\n",
        "# df[['hadm_id', 'llos']].to_csv(f'{root}/llos.csv', index=None)\n",
        "# df_icd[df_icd['hadm_id'].isin(adm_ids)].to_csv(f'{root}/labels_icd.csv', index=None)\n",
        "\n",
        "print(\"further manipulating demo data loaded later in script\")"
      ],
      "metadata": {
        "id": "6byerVU1hlSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a808d6-4ea4-4b39-b101-d0d12e28244f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "further manipulating demo data loaded later in script\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 5: Statistics\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HPenIz6zhylJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from matplotlib.pyplot import plot\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# pd.options.display.float_format = \"{:,.1f}\".format\n",
        "\n",
        "# def cal_demo():\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     df_adm = pd.read_csv(f'{root}/adm_details.csv', parse_dates=['admittime', 'dischtime', 'dob'])\n",
        "#     df_adm['age'] = df_adm['admittime'].subtract(\n",
        "#         df_adm['dob']).dt.days / 365.242\n",
        "#     df_adm['los'] = (df_adm['dischtime'] - df_adm['admittime']\n",
        "#                      ) / np.timedelta64(1, 'D')\n",
        "#     df_adm['gender'] = (df_adm['gender'] == 'M').astype(int)\n",
        "#     result = []\n",
        "#     for task in ['mortality', 'readmit', 'llos']:\n",
        "#         df = pd.read_csv(f'{root}/{task}.csv')\n",
        "#         df = df.merge(df_adm, on='hadm_id', how='left')\n",
        "#         for label in [0, 1]:\n",
        "#             df_part = df[df[task] == label]\n",
        "#             total = len(df_part)\n",
        "#             n_emergency = len(\n",
        "#                 df_part[df_part['admission_type'] == 'EMERGENCY'])\n",
        "#             n_elective = len(df_part[df_part['admission_type'] == 'ELECTIVE'])\n",
        "#             n_urgent = len(df_part[df_part['admission_type'] == 'URGENT'])\n",
        "#             mean_age, std_age = df_part['age'].mean(), df_part['age'].std()\n",
        "#             mean_los, std_los = df_part['los'].mean(), df_part['los'].std()\n",
        "#             result.append([task, label, n_elective, n_emergency,\n",
        "#                            n_urgent, total, mean_age, std_age, mean_los, std_los])\n",
        "#     df_result = pd.DataFrame(result, columns=['task', 'label', 'elective', 'emergency',\n",
        "#                                               'urgent', 'total', 'age (mean)', 'age (std)', 'los (mean)', 'los (std)'])\n",
        "#     print(df_result)\n",
        "\n",
        "\n",
        "# def cal_temporal():\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     images_root = \"/content/drive/MyDrive/mimic-iii_processed_images\"\n",
        "#     df = pd.read_csv(f'{root}/features.csv')\n",
        "#     df_result = df.describe().transpose()\n",
        "#     df_result['missing'] = df.isna().mean()\n",
        "#     print(df_result)\n",
        "\n",
        "\n",
        "# def cal_task_temporal():\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     images_root = \"/content/drive/MyDrive/mimic-iii_processed_images\"\n",
        "#     df_temporal = pd.read_csv(f'{root}/features.csv')\n",
        "#     for task in ['mortality', 'readmit', 'llos']:\n",
        "#         df_label = pd.read_csv(f'{root}/{task}.csv')\n",
        "#         for label in [0, 1]:\n",
        "#             df = df_temporal[df_temporal['hadm_id'].isin(df_label[df_label[task] == label]['hadm_id'])]\n",
        "#             df = df.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).transpose()\n",
        "#             print(task, label)\n",
        "#             print(df)\n",
        "\n",
        "\n",
        "# def plot_los():\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     images_root = \"/content/drive/MyDrive/mimic-iii_processed_images\"\n",
        "#     df = pd.read_csv(f'{root}/los.csv')\n",
        "#     plt.figure(figsize=(8, 4))\n",
        "#     plt.hist(df['los'], bins=60)\n",
        "#     plt.axvline(x=7, color='r', linestyle='-')\n",
        "#     plt.xlabel('Length of stay (day)')\n",
        "#     plt.ylabel('# of patients')\n",
        "#     plt.title('Length of stay distribution of the processed MIMIC-III cohort    ')\n",
        "#     plt.savefig(f'{root}/los_dist.png')\n",
        "\n",
        "\n",
        "# def plot_temporal():\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     images_root = \"/content/drive/MyDrive/mimic-iii_processed_images\"\n",
        "#     df = pd.read_csv(f'{root}/features.csv')\n",
        "#     nrows, ncols = 4, 7\n",
        "#     # plt.figure(figsize=(28, 12))\n",
        "#     plt.clf()\n",
        "#     fig, axs = plt.subplots(nrows, ncols)\n",
        "#     cols = df.columns[2:]\n",
        "#     for i in range(nrows):\n",
        "#         for j in range(ncols):\n",
        "#             if i * ncols + j < len(cols):\n",
        "#                 print(j)\n",
        "#                 col = cols[i * ncols + j]\n",
        "#                 axs[i, j].hist(df[col], bins=20)\n",
        "#                 axs[i, j].title.set_text(col)\n",
        "#     plt.savefig(f'{images_root}/temporal.png')\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # cal_demo()\n",
        "#     # cal_temporal()\n",
        "#     # cal_task_temporal()\n",
        "#     # plot_los()\n",
        "#     plot_temporal()\n"
      ],
      "metadata": {
        "id": "qqj6VIsqhylJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 6: Feature Engineering\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qH2hjzDoh470"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import os\n",
        "# import time\n",
        "# import json\n",
        "# import argparse\n",
        "# from glob import glob\n",
        "\n",
        "\n",
        "# def parse_args():\n",
        "#     root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "#     args = {'data_dir': root}\n",
        "#     return args\n",
        "\n",
        "# def get_time(t):\n",
        "#     try:\n",
        "#         t = float(t)\n",
        "#         return t\n",
        "#     except:\n",
        "#         t = str(t).replace('\"', '')\n",
        "#         t = time.mktime(time.strptime(t,'%Y-%m-%d %H:%M:%S'))\n",
        "#         t = int(t/3600)\n",
        "#         return t\n",
        "\n",
        "# def generate_file_for_each_patient(args, features_csv):\n",
        "#     selected_indices = []\n",
        "#     initial_dir = args['initial_dir']\n",
        "#     os.system('rm -r ' + initial_dir)\n",
        "#     os.mkdir(initial_dir)\n",
        "#     mkdir(initial_dir)\n",
        "#     with open(features_csv, 'r') as f:\n",
        "#       # get length of f\n",
        "#       file_length = sum(1 for line in f)\n",
        "#       print(f'There are {file_length} lines')\n",
        "#       # reset pointer\n",
        "#       f.seek(0)\n",
        "#       for i_line, line in enumerate(f):\n",
        "#         if i_line % 100 == 0:\n",
        "#           print(f'Processing line {i_line} / {file_length}')\n",
        "#         if i_line:\n",
        "#           line_data = line.strip().split(',')\n",
        "#           assert len(line_data) == len(feat_list)\n",
        "#           new_line_data = [line_data[i_feat] for i_feat in selected_indices]\n",
        "#           new_line = ','.join(new_line_data)\n",
        "#           p_file = os.path.join(initial_dir, line_data[0] + '.csv')\n",
        "#           if not os.path.exists(p_file):\n",
        "#             with open(p_file, 'w') as filehandle:\n",
        "#               filehandle.write(new_head)\n",
        "#               filehandle.close()\n",
        "#           filehandle = open(p_file, 'a')\n",
        "#           filehandle.write('\\n' + new_line)\n",
        "#           filehandle.close()\n",
        "#         else:\n",
        "#           feat_list = csv_split(line.strip())\n",
        "#           feat_list = [f.strip('\"') for f in feat_list]\n",
        "#           print('There are {:d} features.'.format(len(feat_list)))\n",
        "#           print(feat_list)\n",
        "#           if len(selected_indices) == 0:\n",
        "#               selected_indices = range(1, len(feat_list))\n",
        "#               selected_feat_list = [feat_list[i_feat].replace('\"','').replace(',', ';') for i_feat in selected_indices]\n",
        "#               new_head = ','.join(selected_feat_list)\n",
        "\n",
        "\n",
        "# def resample_data(args, delta=1, ignore_time=-48):\n",
        "#     resample_dir = args['resample_dir']\n",
        "#     initial_dir = args['initial_dir']\n",
        "\n",
        "#     os.system('rm -r ' + resample_dir)\n",
        "#     os.mkdir(resample_dir)\n",
        "\n",
        "#     count_intervals = [0, 0]\n",
        "#     count_dict = dict()\n",
        "#     two_sets = [set(), set()]\n",
        "#     for i_fi, fi in enumerate(tqdm(os.listdir(initial_dir))):\n",
        "#         time_line_dict = dict()\n",
        "#         for i_line, line in enumerate(open(os.path.join(initial_dir, fi))):\n",
        "#             if i_line:\n",
        "#                 if len(line.strip()) == 0:\n",
        "#                     continue\n",
        "#                 line_data = line.strip().split(',')\n",
        "#                 assert len(line_data) == len(feat_list)\n",
        "#                 ctime = get_time(line_data[0])\n",
        "#                 ctime = delta * int(float(ctime) / delta)\n",
        "#                 if ctime not in time_line_dict:\n",
        "#                     time_line_dict[ctime] = []\n",
        "#                 time_line_dict[ctime].append(line_data)\n",
        "#             else:\n",
        "#                 feat_list = line.strip().split(',')\n",
        "#                 feat_list[0] = 'time'\n",
        "\n",
        "#         with open(os.path.join(resample_dir, fi), 'w') as wf:\n",
        "#           wf.write(','.join(feat_list))\n",
        "#           last_time = None\n",
        "#           vis = 0\n",
        "#           max_t = max(time_line_dict)\n",
        "#           for t in sorted(time_line_dict):\n",
        "#               if t - max_t < ignore_time:\n",
        "#                   continue\n",
        "#               line_list = time_line_dict[t]\n",
        "#               new_line = line_list[0]\n",
        "#               for line_data in line_list:\n",
        "#                   for iv, v in enumerate(line_data):\n",
        "#                       if len(v.strip()):\n",
        "#                           new_line[iv] = v\n",
        "#               new_line[0] = str(t - max_t)\n",
        "#               new_line = '\\n' + ','.join(new_line)\n",
        "#               wf.write(new_line)\n",
        "\n",
        "#               if last_time is not None:\n",
        "#                   delta_t = t - last_time\n",
        "#                   if delta_t > delta:\n",
        "#                       vis = 1\n",
        "#                       count_intervals[0] += 1\n",
        "#                       count_dict[t - last_time] = count_dict.get(t - last_time, 0) + 1\n",
        "#                       two_sets[0].add(fi)\n",
        "#                   two_sets[1].add(fi)\n",
        "#                   count_intervals[1] += 1\n",
        "#               last_time = t\n",
        "#           wf.close()\n",
        "#     print('There are {:d}/{:d} collections data with intervals > {:d}.'.format(count_intervals[0], count_intervals[1], delta))\n",
        "#     print('There are {:d}/{:d} patients with intervals > {:d}.'.format(len(two_sets[0]), len(two_sets[1]), delta))\n",
        "\n",
        "\n",
        "# def generate_feature_dict(args):\n",
        "#     resample_dir = args['resample_dir']\n",
        "#     files = sorted(glob(os.path.join(resample_dir, '*')))\n",
        "#     feature_value_dict = dict()\n",
        "#     feature_missing_dict = dict()\n",
        "#     for ifi, fi in enumerate(tqdm(files)):\n",
        "#         if 'csv' not in fi:\n",
        "#             continue\n",
        "#         for iline, line in enumerate(open(fi)):\n",
        "#             line = line.strip()\n",
        "#             if iline == 0:\n",
        "#                 feat_list = line.split(',')\n",
        "#             else:\n",
        "#                 data = line.split(',')\n",
        "#                 for iv, v in enumerate(data):\n",
        "#                     if v in ['NA', '']:\n",
        "#                         continue\n",
        "#                     else:\n",
        "#                         feat = feat_list[iv]\n",
        "#                         if feat not in feature_value_dict:\n",
        "#                             feature_value_dict[feat] = []\n",
        "#                         feature_value_dict[feat].append(float(v))\n",
        "#     feature_mm_dict = dict()\n",
        "#     feature_ms_dict = dict()\n",
        "\n",
        "#     feature_range_dict = dict()\n",
        "#     len_time = max([len(v) for v in feature_value_dict.values()])\n",
        "#     for feat, vs in feature_value_dict.items():\n",
        "#         vs = sorted(vs)\n",
        "#         value_split = []\n",
        "#         for i in range(args['split_num']):\n",
        "#             n = int(i * len(vs) / args['split_num'])\n",
        "#             value_split.append(vs[n])\n",
        "#         value_split.append(vs[-1])\n",
        "#         feature_range_dict[feat] = value_split\n",
        "\n",
        "\n",
        "#         n = int(len(vs) / args['split_num'])\n",
        "#         feature_mm_dict[feat] = [vs[n], vs[-n - 1]]\n",
        "#         feature_ms_dict[feat] = [np.mean(vs), np.std(vs)]\n",
        "\n",
        "#         feature_missing_dict[feat] = 1.0 - 1.0 * len(vs) / len_time\n",
        "\n",
        "#     json.dump(feature_mm_dict, open(os.path.join(args['files_dir'], 'feature_mm_dict.json'), 'w'))\n",
        "#     json.dump(feature_ms_dict, open(os.path.join(args['files_dir'], 'feature_ms_dict.json'), 'w'))\n",
        "#     json.dump(feat_list, open(os.path.join(args['files_dir'], 'feature_list.json'), 'w'))\n",
        "#     json.dump(feature_missing_dict, open(os.path.join(args['files_dir'], 'feature_missing_dict.json'), 'w'))\n",
        "#     json.dump(feature_range_dict, open(os.path.join(args['files_dir'], 'feature_value_dict_{:d}.json'.format(args['split_num'])), 'w'))\n",
        "\n",
        "\n",
        "# def split_data_to_ten_set(args):\n",
        "#     resample_dir = args['resample_dir']\n",
        "#     files = sorted(glob(os.path.join(resample_dir, '*')))\n",
        "#     np.random.shuffle(files)\n",
        "#     splits = []\n",
        "#     for i in range(10):\n",
        "#         st = int(len(files) * i / 10)\n",
        "#         en = int(len(files) * (i+1) / 10)\n",
        "#         splits.append(files[st:en])\n",
        "#     json.dump(splits, open(os.path.join(args['files_dir'], 'splits.json'), 'w'))\n",
        "\n",
        "\n",
        "# def generate_label_dict(args, task):\n",
        "#     label_dict = dict()\n",
        "#     for i_line, line in enumerate(open(os.path.join(args['data_dir'], '%s.csv' % task))):\n",
        "#         if i_line:\n",
        "#             data = line.strip().split(',')\n",
        "#             pid = data[0]\n",
        "#             label = ''.join(data[1:])\n",
        "#             pid = str(int(float(pid)))\n",
        "#             label_dict[pid] = label\n",
        "#     with open(os.path.join(args['files_dir'], '%s_dict.json' % task), 'w') as json_file:\n",
        "#       json.dump(label_dict, json_file)\n",
        "\n",
        "\n",
        "\n",
        "# def generate_demo_dict(args, demo_csv):\n",
        "#     demo_dict = dict()\n",
        "#     demo_index_dict = dict()\n",
        "#     for i_line, line in enumerate(open(demo_csv)):\n",
        "#         if i_line:\n",
        "#             data = line.strip().split(',')\n",
        "#             pid = str(int(float(data[0])))\n",
        "#             demo_dict[pid] = []\n",
        "#             for demo in data[1:]:\n",
        "#                 if demo not in demo_index_dict:\n",
        "#                     demo_index_dict[demo] = len(demo_index_dict)\n",
        "#                 demo_dict[pid].append(demo_index_dict[demo])\n",
        "#     with open(os.path.join(args['files_dir'], 'demo_dict.json'), 'w') as json_file:\n",
        "#       json.dump(demo_dict, json_file)\n",
        "#     with open(os.path.join(args['files_dir'], 'demo_index_dict.json'), 'w') as json_file:\n",
        "#       json.dump(demo_index_dict, json_file)\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     args = parse_args()\n",
        "#     args['files_dir'] = os.path.join(args['data_dir'], 'files')\n",
        "#     args['initial_dir'] = os.path.join(args['data_dir'], 'initial_data')\n",
        "#     args['resample_dir'] = os.path.join(args['data_dir'], 'resample_dir')\n",
        "#     args['split_num'] = 4000\n",
        "#     print(args.items())\n",
        "\n",
        "#     for x in ['files', 'initial_data', 'resample_dir']:\n",
        "#       if x not in os.listdir(args['data_dir']):\n",
        "#         if x == 'files':\n",
        "#           os.mkdir(args['files_dir'])\n",
        "#         elif x == 'initial_data':\n",
        "#           os.mkdir(args['initial_dir'])\n",
        "#         elif x == 'resample_dir':\n",
        "#           os.mkdir(args['resample_dir'])\n",
        "\n",
        "#     features_csv = os.path.join(args['data_dir'], 'features.csv')\n",
        "#     demo_csv = os.path.join(args['data_dir'], 'demo.csv')\n",
        "#     for task in ['mortality', 'readmit', 'llos']:\n",
        "#         generate_label_dict(args, task)\n",
        "#     generate_demo_dict(args, demo_csv)\n",
        "#     generate_file_for_each_patient(args, features_csv)\n",
        "#     resample_data(args)\n",
        "#     generate_feature_dict(args)\n",
        "#     split_data_to_ten_set(args)\n",
        "\n",
        "# main()"
      ],
      "metadata": {
        "id": "dZAXIyuAh470"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Step 7: Docs2Vec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D16LHyoPh-Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "def parse_args(epochs=None, phase=None):\n",
        "    args = {}\n",
        "    if epochs is None:\n",
        "      args['epochs'] = 30\n",
        "    if phase is None:\n",
        "      args['phase'] = 'infer'\n",
        "    return args\n",
        "\n",
        "\n",
        "processed_root = \"/content/drive/MyDrive/mimic-iii_processed_data\"\n",
        "files_root = \"/content/drive/MyDrive/mimic-iii_processed_data/files\"\n",
        "models_root = \"/content/drive/MyDrive/models\"\n",
        "args = parse_args()\n",
        "#df = pd.read_csv(f'{processed_root}/earlynotes_demo.csv')\n",
        "#df['text'] = df['text'].astype(str).apply(text2words)\n",
        "#print(os.listdir(models_root))"
      ],
      "metadata": {
        "id": "HiuU7Vpth-Oi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PIPELINE**"
      ],
      "metadata": {
        "id": "aB7UKqwUjtTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#vector_dict = json.load(open(f'{args.data_dir}/files/vector_dict.json', 'r'))\n",
        "\n",
        "def find_index(v, vs, i=0, j=-1):\n",
        "    if j == -1:\n",
        "        j = len(vs) - 1\n",
        "\n",
        "    if v > vs[j]:\n",
        "        return j + 1\n",
        "    elif v < vs[i]:\n",
        "        return i\n",
        "    elif j - i == 1:\n",
        "        return j\n",
        "\n",
        "    k = int((i + j)/2)\n",
        "    if v <= vs[k]:\n",
        "        return find_index(v, vs, i, k)\n",
        "    else:\n",
        "        return find_index(v, vs, k, j)\n",
        "\n",
        "\n",
        "class DataBowl(Dataset):\n",
        "    def __init__(self, args, files, phase='train'):\n",
        "        assert (phase == 'train' or phase == 'valid' or phase == 'test')\n",
        "        self.args = args\n",
        "        self.phase = phase\n",
        "        self.files = files\n",
        "        self.feature_mm_dict = json.load(\n",
        "            open(os.path.join(args.files_dir, 'feature_mm_dict.json'), 'r'))\n",
        "        self.feature_value_dict = json.load(open(os.path.join(\n",
        "            args.files_dir, 'feature_value_dict_%d.json' % args.split_num), 'r'))\n",
        "        self.demo_dict = json.load(\n",
        "            open(os.path.join(args.files_dir, 'demo_dict.json'), 'r'))\n",
        "        self.label_dict = json.load(\n",
        "            open(os.path.join(args.files_dir, '%s_dict.json' % args.task), 'r'))\n",
        "\n",
        "        print('Use the last %d collections data' % args.n_visit)\n",
        "\n",
        "    def map_input(self, value, feat_list, feat_index):\n",
        "        index_start = (feat_index + 1) * (1 + self.args.split_num) + 1\n",
        "\n",
        "        if value in ['NA', '']:\n",
        "            return 0\n",
        "        else:\n",
        "            value = float(value)\n",
        "            vs = self.feature_value_dict[feat_list[feat_index]][1:-1]\n",
        "            v = find_index(value, vs) + index_start\n",
        "            return v\n",
        "\n",
        "    def map_output(self, value, feat_list, feat_index):\n",
        "        if value in ['NA', '']:\n",
        "            return 0\n",
        "        else:\n",
        "            value = float(value)\n",
        "            minv, maxv = self.feature_mm_dict[feat_list[feat_index]]\n",
        "            if maxv <= minv:\n",
        "                print(feat_list[feat_index], minv, maxv)\n",
        "            assert maxv > minv\n",
        "            v = (value - minv) / (maxv - minv)\n",
        "            v = max(0, min(v, 1))\n",
        "            return v\n",
        "\n",
        "    def get_mm_item(self, idx):\n",
        "        input_file = self.files[idx]\n",
        "        print(input_file)\n",
        "        pid = input_file.split('/')[-1].split('.')[0]\n",
        "\n",
        "        if input_file in args.resample_dir:\n",
        "          with open(input_file) as f:\n",
        "              input_data = f.read().strip().split('\\n')\n",
        "        else:\n",
        "          input_data = []\n",
        "\n",
        "        time_list, input_list = [], []\n",
        "\n",
        "        for iline in range(len(input_data)):\n",
        "            inp = input_data[iline].strip()\n",
        "            if iline == 0:\n",
        "                feat_list = inp.split(',')\n",
        "            else:\n",
        "                in_vs = inp.split(',')\n",
        "                ctime = int(inp.split(',')[0])\n",
        "                input = []\n",
        "                for i, iv in enumerate(in_vs):\n",
        "                    if self.args.use_ve:\n",
        "                        input.append(self.map_input(iv, feat_list, i))\n",
        "                    else:\n",
        "                        input.append(self.map_output(iv, feat_list, i))\n",
        "                input_list.append(input)\n",
        "                time_list.append(- int(ctime))\n",
        "\n",
        "        if len(input_list) < self.args.n_visit:\n",
        "            for _ in range(self.args.n_visit - len(input_list)):\n",
        "                # pad empty visit\n",
        "                vs = [0 for _ in range(self.args.input_size + 1)]\n",
        "                input_list = [vs] + input_list\n",
        "                time_list = [time_list[0]] + time_list\n",
        "        else:\n",
        "            if self.use_first_records:\n",
        "                input_list = input_list[: self.args.n_visit]\n",
        "                time_list = time_list[: self.args.n_visit]\n",
        "            else:\n",
        "                input_list = input_list[-self.args.n_visit:]\n",
        "                time_list = time_list[-self.args.n_visit:]\n",
        "\n",
        "        if self.args.value_embedding == 'no' or self.args.use_ve == 0:\n",
        "            input_list = np.array(input_list, dtype=np.float32)\n",
        "        else:\n",
        "            input_list = np.array(input_list, dtype=np.int64)\n",
        "        time_list = np.array(time_list, dtype=np.int64) + 1\n",
        "        assert time_list.min() >= 0\n",
        "        if self.args.value_embedding != 'no':\n",
        "            input_list = input_list[:, 1:]\n",
        "        else:\n",
        "            input_list = input_list.transpose()\n",
        "\n",
        "        label = np.array([int(l)\n",
        "                          for l in self.label_dict[pid]], dtype=np.float32)\n",
        "        # demo = np.array([self.demo_dict[pid] for _ in range(self.args.n_visit)], dtype=np.int64)\n",
        "        demo = np.array(self.demo_dict.get(pid, 0), dtype=np.int64)\n",
        "\n",
        "        # content = self.unstructure_dict.get(pid, [])\n",
        "        # while len(content) < self.max_length:\n",
        "        #     content.append(0)\n",
        "        # content = content[: self.max_length]\n",
        "        # content = np.array(content, dtype=np.int64)\n",
        "        content = vector_dict[pid]\n",
        "        while len(content) < 12:\n",
        "            content.append([0] * 200)\n",
        "        content = content[:12]\n",
        "        content = np.array(content, dtype=np.float32)\n",
        "        # content = np.mean(content, axis=0)\n",
        "\n",
        "        return torch.from_numpy(input_list), torch.from_numpy(time_list), torch.from_numpy(demo), torch.from_numpy(content), torch.from_numpy(label), input_file\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.get_mm_item(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "metadata": {
        "id": "zdFjrVYAkGcv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **METHODOLOGY (MODEL)**\n",
        "\n",
        "### Model Description\n",
        "The research introduces a multi-modal neural network architecture designed to enhance predictive modeling by integrating both structured and unstructured data from Electronic Health Records (EHRs). The model capitalizes on the strengths of convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) to process and learn from the temporal dynamics and textual complexity found within clinical health datasets.\n",
        "\n",
        "1. **Components**:\n",
        "   - **Static Information Encoder**: Encodes static categorical features such as patient demos and admission-related information by way of one-hot encoding.\n",
        "   - **Temporal Signals**: Utilize a sequential layer (CNN and/or LSTM) to model time-series clinical data, extracting vital patterns and signals from medical measurements.\n",
        "   - **Sequential Notes Representation**: Use of document embeddings combined with sequential neural network architecture to handle long sequences found in clinical notes, enhancing the ability to capture relevant medical contexts and details.\n",
        "\n",
        "2. **Data Fusion**:\n",
        "   - The model integrates the processed data streams into a unified patient representation, which combines the encoded static information, temporal features, and finally the text-based insights.\n"
      ],
      "metadata": {
        "id": "FmtN2NFSEbYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "gckuoEDLF9hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def value_embedding_data(d = 200, split = 200):\n",
        "    vec = np.array([np.arange(split) * i for i in range(int(d/2))], dtype=np.float32).transpose()\n",
        "    vec = vec / vec.max()\n",
        "    embedding = np.concatenate((np.sin(vec), np.cos(vec)), 1)\n",
        "    embedding[0, :d] = 0\n",
        "    embedding = torch.from_numpy(embedding)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        # unstructure\n",
        "        if args.use_unstructure:\n",
        "            self.vocab_embedding = nn.Embedding (args.unstructure_size, args.embed_size )\n",
        "            self.vocab_lstm = nn.LSTM ( input_size=args.embed_size,\n",
        "                              # hidden_size=args.hidden_size,\n",
        "                              hidden_size=1,\n",
        "                              num_layers=args.num_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "            self.vocab_mapping = nn.Sequential(\n",
        "                    nn.Linear(args.embed_size * 2, args.embed_size),\n",
        "                    nn.ReLU ( ),\n",
        "                    nn.Dropout ( 0.1),\n",
        "                    nn.Linear(args.embed_size, args.embed_size),\n",
        "                    )\n",
        "            self.cat_output = nn.Sequential (\n",
        "                    nn.Linear (args.rnn_size * 3, args.rnn_size),\n",
        "                    nn.ReLU ( ),\n",
        "                    nn.Dropout ( 0.1),\n",
        "                    nn.Linear ( args.rnn_size, output_size),\n",
        "                    )\n",
        "            self.cat_output = nn.Sequential (\n",
        "                    nn.ReLU ( ),\n",
        "                    nn.Dropout ( 0.1),\n",
        "                    nn.Linear (args.rnn_size * 3, output_size),\n",
        "                    )\n",
        "\n",
        "        if args.value_embedding == 'no':\n",
        "            self.embedding = nn.Linear(args.input_size, args.embed_size)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding (args.vocab_size, args.embed_size )\n",
        "        self.lstm1 = nn.LSTM (input_size=args.embed_size,\n",
        "                              hidden_size=args.hidden_size,\n",
        "                              num_layers=args.num_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM (input_size=args.embed_size,\n",
        "                              hidden_size=args.hidden_size,\n",
        "                              num_layers=args.num_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "        self.dd_embedding = nn.Embedding (args.n_ehr, args.embed_size )\n",
        "        self.value_embedding = nn.Embedding.from_pretrained(value_embedding_data(args.embed_size, args.split_num + 1))\n",
        "        self.value_mapping = nn.Sequential(\n",
        "                nn.Linear ( args.embed_size * 2, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout ( 0.1),\n",
        "                )\n",
        "        self.dd_mapping = nn.Sequential(\n",
        "                nn.Linear ( args.embed_size, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear ( args.embed_size, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout(0.1),\n",
        "                )\n",
        "        self.dx_mapping = nn.Sequential(\n",
        "                nn.Linear ( args.embed_size * 2, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Linear ( args.embed_size, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                )\n",
        "\n",
        "        self.tv_mapping = nn.Sequential (\n",
        "            nn.Linear ( args.embed_size * 2, args.embed_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Linear ( args.embed_size, args.embed_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.1),\n",
        "        )\n",
        "        self.relu = nn.ReLU ( )\n",
        "\n",
        "        lstm_size = args.rnn_size\n",
        "\n",
        "        lstm_size *= 2\n",
        "        self.output_mapping = nn.Sequential (\n",
        "            nn.Linear (lstm_size, args.rnn_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Linear (args.rnn_size, args.rnn_size),\n",
        "            nn.ReLU ( )\n",
        "        )\n",
        "\n",
        "        self.output = nn.Sequential (\n",
        "            nn.Linear (args.rnn_size * 2, args.rnn_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.1),\n",
        "            nn.Linear ( args.rnn_size, output_size),\n",
        "        )\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        self.one_output = nn.Sequential (\n",
        "                # nn.Linear (args.embed_size * 3, args.embed_size),\n",
        "                # nn.ReLU ( ),\n",
        "                nn.Dropout ( 0.1),\n",
        "                nn.Linear ( args.embed_size, output_size),\n",
        "            )\n",
        "\n",
        "\n",
        "    def visit_pooling(self, x):\n",
        "        output = x\n",
        "        size = output.size()\n",
        "        output = output.view(size[0] * size[1], size[2], output.size(3))    # (64*30, 13, 200)\n",
        "        output = torch.transpose(output, 1,2).contiguous()                  # (64*30, 200, 13)\n",
        "        output = self.pooling(output)                                       # (64*30, 200, 1)\n",
        "        output = output.view(size[0], size[1], size[3])                     # (64, 30, 200)\n",
        "        return output\n",
        "\n",
        "    def value_order_embedding(self, x):\n",
        "        size = list(x[0].size())               # (64, 30, 13)\n",
        "        index, value = x\n",
        "        xi = self.embedding(index.view(-1))          # (64*30*13, 200)\n",
        "        # xi = xi * (value.view(-1).float() + 1.0 / self.args.split_num)\n",
        "        xv = self.value_embedding(value.view(-1))    # (64*30*13, 200)\n",
        "        x = torch.cat((xi, xv), 1)                   # (64*30*13, 1024)\n",
        "        x = self.value_mapping(x)                    # (64*30*13, 200)\n",
        "        size.append(-1)\n",
        "        x = x.view(size)                    # (64, 30, 13, 200)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, t, dd, content=None):\n",
        "\n",
        "        if 0 and content is not None:\n",
        "            content, _ = self.lstm1(content)\n",
        "            content = self.vocab_mapping(content)\n",
        "            content = torch.transpose(content, 1, 2).contiguous()\n",
        "            content = self.pooling(content)\n",
        "            content = content.view((content.size(0), -1))\n",
        "            return self.one_output(content)\n",
        "\n",
        "        # value embedding\n",
        "        x = self.value_order_embedding(x)\n",
        "        x = self.visit_pooling(x)\n",
        "\n",
        "        # demo embedding\n",
        "        dsize = list(dd.size()) + [-1]\n",
        "        d = self.dd_embedding(dd.view(-1)).view(dsize)\n",
        "        d = self.dd_mapping(d)\n",
        "        d = torch.transpose(d, 1,2).contiguous()                  # (64*30, 200, 100)\n",
        "        d = self.pooling(d)\n",
        "        d = d.view((d.size(0), -1))\n",
        "\n",
        "        # x = torch.cat((x, d), 2)\n",
        "        # x = self.dx_mapping(x)\n",
        "\n",
        "        # time embedding\n",
        "        # t = self.value_embedding(t)\n",
        "        # x = self.tv_mapping(torch.cat((x, t), 2))\n",
        "\n",
        "        # lstm\n",
        "        lstm_out, _ = self.lstm2( x )            # (64, 30, 1024)\n",
        "        output = self.output_mapping(lstm_out)\n",
        "        output = torch.transpose(output, 1,2).contiguous()                  # (64*30, 200, 100)\n",
        "        # print('ouput.size', output.size())\n",
        "        output = self.pooling(output)                                       # (64*30, 200, 1)\n",
        "        output = output.view((output.size(0), -1))\n",
        "        out = self.output(torch.cat((output, d), 1))\n",
        "\n",
        "        # unstructure\n",
        "        if content is not None:\n",
        "            # print(content.size())   # [64, 1000]\n",
        "            content, _ = self.lstm1(content)\n",
        "            content = self.vocab_mapping(content)\n",
        "            content = torch.transpose(content, 1, 2).contiguous()\n",
        "            content = self.pooling(content)\n",
        "            content = content.view((content.size(0), -1))\n",
        "            out = self.cat_output(torch.cat((output, content, d), 1))\n",
        "\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "5KFw4b-dEd3F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "F9rgLySgGDwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import *\n",
        "import numpy as np\n",
        "import sys\n",
        "output_size = 1\n",
        "\n",
        "def value_embedding_data(d = 200, split = 200):\n",
        "    vec = np.array([np.arange(split) * i for i in range(int(d/2))], dtype=np.float32).transpose()\n",
        "    vec = vec / vec.max()\n",
        "    embedding = np.concatenate((np.sin(vec), np.cos(vec)), 1)\n",
        "    embedding[0, :d] = 0\n",
        "    embedding = torch.from_numpy(embedding)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def conv3(in_channels, out_channels, stride=1, kernel_size=3):\n",
        "    return nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                     stride=stride, padding=1, bias=False)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNN, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "\n",
        "        if args.value_embedding == 'no':\n",
        "            self.embedding = nn.Linear(args.input_size, args.embed_size)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding (args.vocab_size, args.embed_size )\n",
        "        self.dd_embedding = nn.Embedding (args.n_ehr, args.embed_size )\n",
        "        self.value_embedding = nn.Embedding.from_pretrained(value_embedding_data(args.embed_size, args.split_num + 1))\n",
        "        self.value_mapping = nn.Sequential(\n",
        "                nn.Linear ( args.embed_size * 2, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout ( 0.1),\n",
        "                )\n",
        "        self.dd_mapping = nn.Sequential(\n",
        "                nn.Linear ( args.embed_size, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear ( args.embed_size, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout(0.1),\n",
        "                )\n",
        "        self.dx_mapping = nn.Sequential(\n",
        "                nn.Linear ( args.embed_size * 2, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Linear ( args.embed_size, args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                )\n",
        "\n",
        "        self.tv_mapping = nn.Sequential (\n",
        "            nn.Linear ( args.embed_size * 2, args.embed_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Linear ( args.embed_size, args.embed_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.1),\n",
        "        )\n",
        "        self.relu = nn.ReLU ( )\n",
        "\n",
        "        lstm_size = args.rnn_size\n",
        "\n",
        "        lstm_size *= 2\n",
        "        self.output_mapping = nn.Sequential (\n",
        "            nn.Linear (lstm_size, args.rnn_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Linear (args.rnn_size, args.rnn_size),\n",
        "            nn.ReLU ( )\n",
        "        )\n",
        "\n",
        "        self.cat_output = nn.Sequential (\n",
        "            nn.Linear (args.rnn_size * 2, args.rnn_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.1),\n",
        "            nn.Linear ( args.rnn_size, output_size),\n",
        "        )\n",
        "        self.output = nn.Sequential (\n",
        "            nn.Linear (args.rnn_size, args.rnn_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.1),\n",
        "            nn.Linear ( args.rnn_size, output_size),\n",
        "        )\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "\n",
        "        layers = [1, 2, 2]\n",
        "        embed_size = args.embed_size\n",
        "        block = ResidualBlock\n",
        "        self.in_channels = embed_size\n",
        "        self.bn1 = nn.BatchNorm1d(embed_size)\n",
        "        self.bn2 = nn.BatchNorm1d(embed_size)\n",
        "        self.bn3 = nn.BatchNorm1d(embed_size)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, embed_size, layers[0], 2)\n",
        "        self.layer2 = self.make_layer(block, embed_size, layers[1], 2)\n",
        "        self.layer3 = self.make_layer(block, embed_size, layers[2], 2)\n",
        "\n",
        "\n",
        "\n",
        "        # unstructure\n",
        "        if args.use_unstructure:\n",
        "            self.vocab_embedding = nn.Embedding (args.unstructure_size+10, args.embed_size )\n",
        "            # self.vocab_layer = self.make_layer(block, embed_size, layers[0], 2)\n",
        "            self.vocab_layer = nn.Sequential(\n",
        "                    nn.Dropout(0.2),\n",
        "                    conv3(embed_size, embed_size, 2, 2),\n",
        "                    nn.BatchNorm1d(embed_size),\n",
        "                    nn.Dropout(0.2),\n",
        "                    nn.ReLU(),\n",
        "                    # conv3(embed_size, embed_size, 2, 3),\n",
        "                    # nn.BatchNorm1d(embed_size),\n",
        "                    # nn.Dropout(0.1),\n",
        "                    # nn.ReLU(),\n",
        "                    # conv3(embed_size, embed_size, 2, 3),\n",
        "                    # nn.BatchNorm1d(embed_size),\n",
        "                    # nn.Dropout(0.1),\n",
        "                    # nn.ReLU(),\n",
        "                    )\n",
        "            self.vocab_output = nn.Sequential (\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout ( 0.1),\n",
        "                nn.Linear (args.embed_size * 3, 3 * args.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "                nn.Dropout ( 0.1),\n",
        "                nn.Linear ( 3 * args.embed_size, output_size),\n",
        "            )\n",
        "            self.one_output = nn.Sequential (\n",
        "                # nn.Linear (args.embed_size * 3, args.embed_size),\n",
        "                # nn.ReLU ( ),\n",
        "                nn.Dropout ( 0.1),\n",
        "                nn.Linear ( args.embed_size, output_size),\n",
        "            )\n",
        "\n",
        "\n",
        "    def visit_pooling(self, x):\n",
        "        output = x\n",
        "        size = output.size()\n",
        "        output = output.view(size[0] * size[1], size[2], output.size(3))    # (64*30, 13, 200)\n",
        "        output = torch.transpose(output, 1,2).contiguous()                  # (64*30, 200, 13)\n",
        "        output = self.pooling(output)                                       # (64*30, 200, 1)\n",
        "        output = output.view(size[0], size[1], size[3])                     # (64, 30, 200)\n",
        "        return output\n",
        "\n",
        "    def value_order_embedding(self, x):\n",
        "        size = list(x[0].size())               # (64, 30, 13)\n",
        "        index, value = x\n",
        "        xi = self.embedding(index.view(-1))          # (64*30*13, 200)\n",
        "        # xi = xi * (value.view(-1).float() + 1.0 / self.args.split_num)\n",
        "        xv = self.value_embedding(value.view(-1))    # (64*30*13, 200)\n",
        "        x = torch.cat((xi, xv), 1)                   # (64*30*13, 1024)\n",
        "        x = self.value_mapping(x)                    # (64*30*13, 200)\n",
        "        size.append(-1)\n",
        "        x = x.view(size)                    # (64, 30, 13, 200)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm1d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, t, dd, content=None):\n",
        "\n",
        "        if content is not None:\n",
        "            # content = self.vocab_embedding(content).transpose(1,2)\n",
        "            content = self.vocab_layer(content.transpose(1,2))\n",
        "            content = self.pooling(content)                                       # (64*30, 200, 1)\n",
        "            content = content.view((content.size(0), -1))\n",
        "            return self.one_output(content)\n",
        "\n",
        "        # value embedding\n",
        "        x = self.value_order_embedding(x)\n",
        "        x = self.visit_pooling(x)\n",
        "\n",
        "\n",
        "        # time embedding\n",
        "        # t = self.value_embedding(t)\n",
        "        # x = self.tv_mapping(torch.cat((x, t), 2))\n",
        "\n",
        "        # cnn\n",
        "        x = torch.transpose(x, 1, 2,).contiguous()\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        # out = self.layer2(out)\n",
        "        # out = self.layer3(out)\n",
        "\n",
        "        output = self.pooling(out)                                       # (64*30, 200, 1)\n",
        "        output = output.view((output.size(0), -1))\n",
        "\n",
        "\n",
        "        if len(dd.size()) > 1:\n",
        "            # demo embedding\n",
        "            dsize = list(dd.size()) + [-1]\n",
        "            d = self.dd_embedding(dd.view(-1)).view(dsize)\n",
        "            d = self.dd_mapping(d)\n",
        "            d = torch.transpose(d, 1,2).contiguous()                  # (64*30, 200, 100)\n",
        "            d = self.pooling(d)\n",
        "            d = d.view((d.size(0), -1))\n",
        "            output = torch.cat((output, d), 1)\n",
        "            out = self.cat_output(output)\n",
        "        # else:\n",
        "        #     out = self.output(output)\n",
        "\n",
        "        if content is not None:\n",
        "            # content = self.vocab_embedding(content)\n",
        "            content = self.vocab_layer(content.transpose(1,2))\n",
        "            content = self.pooling(content)                                       # (64*30, 200, 1)\n",
        "            content = content.view((content.size(0), -1))\n",
        "            # content = self.one_output(content) + 0.3 * out\n",
        "            # out = content + out\n",
        "            # out = content\n",
        "            output = torch.cat((output, content), 1)\n",
        "            out = self.vocab_output(output)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ADK1u7pMGGoi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CRITERION DEFINTION (& OTHER RELATIVE METRICS)**"
      ],
      "metadata": {
        "id": "cotrvs6Zkaxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hard_mining(neg_output, neg_labels, num_hard, largest=True):\n",
        "    num_hard = min(max(num_hard, 10), len(neg_output))\n",
        "    _, idcs = torch.topk(neg_output, min(num_hard, len(neg_output)), largest=largest)\n",
        "    neg_output = torch.index_select(neg_output, 0, idcs)\n",
        "    neg_labels = torch.index_select(neg_labels, 0, idcs)\n",
        "    return neg_output, neg_labels\n",
        "\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    def __init__(self, hard_mining):\n",
        "        super(Loss, self).__init__()\n",
        "        self.classify_loss = nn.BCELoss()\n",
        "        self.hard_mining = hard_mining\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, prob, labels, train=True):\n",
        "\n",
        "        prob = self.sigmoid(prob)\n",
        "\n",
        "        pos_ind = labels > 0.5\n",
        "        neg_ind = labels < 0.5\n",
        "        pos_label = labels[pos_ind]\n",
        "        neg_label = labels[neg_ind]\n",
        "        pos_prob = prob[pos_ind]\n",
        "        neg_prob = prob[neg_ind]\n",
        "        pos_loss, neg_loss = 0, 0\n",
        "\n",
        "        # hard mining\n",
        "        num_hard_pos = 10\n",
        "        num_hard_neg = 18\n",
        "        if self.hard_mining:\n",
        "            pos_prob, pos_label= hard_mining(pos_prob, pos_label, num_hard_pos, largest=False)\n",
        "            neg_prob, neg_label= hard_mining(neg_prob, neg_label, num_hard_neg, largest=True)\n",
        "\n",
        "        if len(pos_prob):\n",
        "            pos_loss = 0.5 * self.classify_loss(pos_prob, pos_label)\n",
        "\n",
        "        if len(neg_prob):\n",
        "            neg_loss = 0.5 * self.classify_loss(neg_prob, neg_label)\n",
        "        classify_loss = pos_loss + neg_loss\n",
        "\n",
        "        prob = prob.data.cpu().numpy() > 0.5\n",
        "        labels = labels.data.cpu().numpy()\n",
        "        pos_l = (labels==1).sum()\n",
        "        neg_l = (labels==0).sum()\n",
        "        pos_p = (prob + labels == 2).sum()\n",
        "        neg_p = (prob + labels == 0).sum()\n",
        "\n",
        "        return [classify_loss, pos_p, pos_l, neg_p, neg_l]\n",
        "\n",
        "\n",
        "class MultiClassLoss(nn.Module):\n",
        "    def __init__(self, hard_mining):\n",
        "        super(MultiClassLoss, self).__init__()\n",
        "        self.classify_loss = nn.BCELoss()\n",
        "        self.hard_mining = hard_mining\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, prob, labels, train=True):\n",
        "        prob = self.sigmoid(prob)\n",
        "        classify_loss, pos_p, pos_l, neg_p, neg_l = 0, 0, 0, 0, 0\n",
        "\n",
        "        prob_list = prob\n",
        "        labels_list = labels\n",
        "        for i in range(prob.size(1)):\n",
        "            prob = prob_list[:, i]\n",
        "            labels = labels_list[:, i]\n",
        "\n",
        "            pos_ind = labels > 0.5\n",
        "            neg_ind = labels < 0.5\n",
        "            pos_label = labels[pos_ind]\n",
        "            neg_label = labels[neg_ind]\n",
        "            pos_prob = prob[pos_ind]\n",
        "            neg_prob = prob[neg_ind]\n",
        "            pos_loss, neg_loss = 0, 0\n",
        "\n",
        "            # hard mining\n",
        "            num_hard_pos = 10\n",
        "            num_hard_neg = 18\n",
        "            if self.hard_mining:\n",
        "                pos_prob, pos_label= hard_mining(pos_prob, pos_label, num_hard_pos, largest=False)\n",
        "                neg_prob, neg_label= hard_mining(neg_prob, neg_label, num_hard_neg, largest=True)\n",
        "\n",
        "            if len(pos_prob):\n",
        "                pos_loss = 0.5 * self.classify_loss(pos_prob, pos_label)\n",
        "\n",
        "            if len(neg_prob):\n",
        "                neg_loss = 0.5 * self.classify_loss(neg_prob, neg_label)\n",
        "\n",
        "            classify_loss = classify_loss + pos_loss + neg_loss\n",
        "\n",
        "            # stati number\n",
        "            prob = prob.data.cpu().numpy() > 0.5\n",
        "            labels = labels.data.cpu().numpy()\n",
        "            pos_l += (labels==1).sum()\n",
        "            neg_l += (labels==0).sum()\n",
        "            pos_p += (prob + labels == 2).sum()\n",
        "            neg_p += (prob + labels == 0).sum()\n",
        "\n",
        "        return [classify_loss, pos_p, pos_l, neg_p, neg_l]"
      ],
      "metadata": {
        "id": "dFUS2mIYkpfe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def compute_nRMSE(pred, label, mask):\n",
        "    '''\n",
        "    same as 3dmice\n",
        "    '''\n",
        "    assert pred.shape == label.shape == mask.shape\n",
        "\n",
        "    missing_indices = mask==1\n",
        "    missing_pred = pred[missing_indices]\n",
        "    missing_label = label[missing_indices]\n",
        "    missing_rmse = np.sqrt(((missing_pred - missing_label) ** 2).mean())\n",
        "\n",
        "    init_indices = mask==0\n",
        "    init_pred = pred[init_indices]\n",
        "    init_label = label[init_indices]\n",
        "    init_rmse = np.sqrt(((init_pred - init_label) ** 2).mean())\n",
        "\n",
        "    metric_list = [missing_rmse, init_rmse]\n",
        "    for i in range(pred.shape[2]):\n",
        "        apred = pred[:,:,i]\n",
        "        alabel = label[:,:, i]\n",
        "        amask = mask[:,:, i]\n",
        "\n",
        "        mrmse, irmse = [], []\n",
        "        for ip in range(len(apred)):\n",
        "            ipred = apred[ip]\n",
        "            ilabel = alabel[ip]\n",
        "            imask = amask[ip]\n",
        "\n",
        "            x = ilabel[imask>=0]\n",
        "            if len(x) == 0:\n",
        "                continue\n",
        "\n",
        "            minv = ilabel[imask>=0].min()\n",
        "            maxv = ilabel[imask>=0].max()\n",
        "            if maxv == minv:\n",
        "                continue\n",
        "\n",
        "            init_indices = imask==0\n",
        "            init_pred = ipred[init_indices]\n",
        "            init_label = ilabel[init_indices]\n",
        "\n",
        "            missing_indices = imask==1\n",
        "            missing_pred = ipred[missing_indices]\n",
        "            missing_label = ilabel[missing_indices]\n",
        "\n",
        "            assert len(init_label) + len(missing_label) >= 2\n",
        "\n",
        "            if len(init_pred) > 0:\n",
        "                init_rmse = np.sqrt((((init_pred - init_label) / (maxv - minv)) ** 2).mean())\n",
        "                irmse.append(init_rmse)\n",
        "\n",
        "            if len(missing_pred) > 0:\n",
        "                missing_rmse = np.sqrt((((missing_pred - missing_label)/ (maxv - minv)) ** 2).mean())\n",
        "                mrmse.append(missing_rmse)\n",
        "\n",
        "        metric_list.append(np.mean(mrmse))\n",
        "        metric_list.append(np.mean(irmse))\n",
        "\n",
        "    metric_list = np.array(metric_list)\n",
        "\n",
        "\n",
        "    metric_list[0] = np.mean(metric_list[2:][::2])\n",
        "    metric_list[1] = np.mean(metric_list[3:][::2])\n",
        "\n",
        "    return metric_list\n",
        "\n",
        "\n",
        "def save_model(p_dict):\n",
        "    args = p_dict['args']\n",
        "    model = p_dict['model']\n",
        "    state_dict = model.state_dict()\n",
        "    for key in state_dict.keys():\n",
        "        state_dict[key] = state_dict[key].cpu()\n",
        "    all_dict = {\n",
        "            'epoch': p_dict['epoch'],\n",
        "            'args': p_dict['args'],\n",
        "            'best_metric': p_dict['best_metric'],\n",
        "            'state_dict': state_dict\n",
        "            }\n",
        "    torch.save(all_dict, args.model_path)\n",
        "\n",
        "def load_model(p_dict, model_file):\n",
        "    all_dict = torch.load(model_file)\n",
        "    p_dict['epoch'] = all_dict['epoch']\n",
        "    # p_dict['args'] = all_dict['args']\n",
        "    p_dict['best_metric'] = all_dict['best_metric']\n",
        "    # for k,v in all_dict['state_dict'].items():\n",
        "    #     p_dict['model_dict'][k].load_state_dict(all_dict['state_dict'][k])\n",
        "    p_dict['model'].load_state_dict(all_dict['state_dict'])\n",
        "\n",
        "def compute_auc(labels, probs):\n",
        "    fpr, tpr, thr = metrics.roc_curve(labels, probs)\n",
        "    return metrics.auc(fpr, tpr)\n",
        "\n",
        "def compute_metric(labels, probs):\n",
        "    labels = np.array(labels)\n",
        "    probs = np.array(probs)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(labels, probs)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "    aupr = metrics.average_precision_score(labels, probs)\n",
        "    optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "    preds = [1 if prob >= optimal_threshold else 0 for prob in probs]\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(labels, preds).ravel()\n",
        "    precision = 1.0 * (tp / (tp + fp))\n",
        "    sen = 1.0 * (tp / (tp + fn))  # recall\n",
        "    spec = 1.0 * (tn / (tn + fp))\n",
        "    f1 = metrics.f1_score(labels, preds)\n",
        "    return precision, sen, spec, f1, auc, aupr"
      ],
      "metadata": {
        "id": "ZVbDkeQFl8lE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_metric(y_true, probs):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, probs)\n",
        "    optimal_idx = np.argmax(np.sqrt(tpr * (1-fpr)))\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    preds = (probs > optimal_threshold).astype(int)\n",
        "    auc = metrics.roc_auc_score(y_true, probs)\n",
        "    auprc = metrics.average_precision_score(y_true, probs)\n",
        "    f1 = metrics.f1_score(y_true, preds)\n",
        "    return f1, auc, auprc\n"
      ],
      "metadata": {
        "id": "2NY70yzLl_Ta"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **METHODOLOGY (TRAINING)**"
      ],
      "metadata": {
        "id": "lpuwHYioEPD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Argument Parsing"
      ],
      "metadata": {
        "id": "davmx0qnjh_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='clinical fusion help')\n",
        "parser.add_argument(\n",
        "        '--data-dir',\n",
        "        type=str,\n",
        "        default='/content/drive/MyDrive/mimic-iii_processed_data/',\n",
        "        help='selected and preprocessed data directory'\n",
        "        )\n",
        "\n",
        "# problem setting\n",
        "parser.add_argument('--task',\n",
        "        default='mortality',\n",
        "        type=str,\n",
        "        metavar='S',\n",
        "        help='start from checkpoints')\n",
        "parser.add_argument(\n",
        "        '--last-time',\n",
        "        metavar='last event time',\n",
        "        type=int,\n",
        "        default=-4,\n",
        "        help='last time'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--time-range',\n",
        "        default=10000,\n",
        "        type=int)\n",
        "parser.add_argument(\n",
        "        '--n-code',\n",
        "        default=8,\n",
        "        type=int,\n",
        "        help='at most n codes for same visit')\n",
        "parser.add_argument(\n",
        "        '--n-visit',\n",
        "        default=24,\n",
        "        type=int,\n",
        "        help='at most input n visits')\n",
        "\n",
        "\n",
        "\n",
        "# method seetings\n",
        "parser.add_argument(\n",
        "        '--model',\n",
        "        '-m',\n",
        "        type=str,\n",
        "        default='lstm',\n",
        "        help='model'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--split-num',\n",
        "        metavar='split num',\n",
        "        type=int,\n",
        "        default=4000,\n",
        "        help='split num'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--split-nor',\n",
        "        metavar='split normal range',\n",
        "        type=int,\n",
        "        default=200,\n",
        "        help='split num'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-glp',\n",
        "        metavar='use global pooling operation',\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help='use global pooling operation'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-value',\n",
        "        metavar='use value embedding as input',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use value embedding as input'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-cat',\n",
        "        metavar='use cat for time and value embedding',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use cat or add'\n",
        "        )\n",
        "\n",
        "\n",
        "# model parameters\n",
        "parser.add_argument(\n",
        "        '--embed-size',\n",
        "        metavar='EMBED SIZE',\n",
        "        type=int,\n",
        "        default=512,\n",
        "        help='embed size'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--rnn-size',\n",
        "        metavar='rnn SIZE',\n",
        "        type=int,\n",
        "        help='rnn size'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--hidden-size',\n",
        "        metavar='hidden SIZE',\n",
        "        type=int,\n",
        "        help='hidden size'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--num-layers',\n",
        "        metavar='num layers',\n",
        "        type=int,\n",
        "        default=2,\n",
        "        help='num layers'\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# traing process setting\n",
        "parser.add_argument('--phase',\n",
        "        default='train',\n",
        "        type=str,\n",
        "        help='train/test phase')\n",
        "parser.add_argument(\n",
        "        '--batch-size',\n",
        "        '-b',\n",
        "        metavar='BATCH SIZE',\n",
        "        type=int,\n",
        "        default=64,\n",
        "        help='batch size'\n",
        "        )\n",
        "parser.add_argument('--model-path', type=str, default='models/best.ckpt', help='model path')\n",
        "parser.add_argument('--resume',\n",
        "        default='',\n",
        "        type=str,\n",
        "        metavar='S',\n",
        "        help='start from checkpoints')\n",
        "parser.add_argument(\n",
        "        '--workers',\n",
        "        default=8,\n",
        "        type=int,\n",
        "        metavar='N',\n",
        "        help='number of data loading workers (default: 32)')\n",
        "parser.add_argument('--lr',\n",
        "        '--learning-rate',\n",
        "        default=0.0001,\n",
        "        type=float,\n",
        "        metavar='LR',\n",
        "        help='initial learning rate')\n",
        "parser.add_argument('--epochs',\n",
        "        default=50,\n",
        "        type=int,\n",
        "        metavar='N',\n",
        "        help='number of total epochs to run')\n",
        "\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "#args.data_dir = os.path.join(args.data_dir, 'processed')\n",
        "args.files_dir = os.path.join(args.data_dir, 'files')\n",
        "args.resample_dir = os.path.join(args.data_dir, 'resample_dir')\n",
        "args.initial_dir = os.path.join(args.data_dir, 'initial_data')\n",
        "args.embed_size = 200\n",
        "args.hidden_size = args.rnn_size = args.embed_size\n",
        "if torch.cuda.is_available():\n",
        "    args.gpu = 1\n",
        "else:\n",
        "    args.gpu = 0\n",
        "args.use_ve = 1\n",
        "args.n_visit = 24\n",
        "args.use_unstructure = 0\n",
        "args.value_embedding = 'use_order'\n",
        "# args.value_embedding = 'no'\n",
        "print ('epochs,', args.epochs)\n",
        "\n",
        "args.task = 'mortality'\n",
        "args.files_dir = args.files_dir\n",
        "args.data_dir = args.data_dir"
      ],
      "metadata": {
        "id": "B9bng3wcjhh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef39a67-385d-438b-c829-801756d2e215"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs, 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = True\n",
        "if not demo:\n",
        "  epochs = args['epochs']\n",
        "  splits = range(10)\n",
        "  data = json.load(open(f'{files_root}/splits.json'))\n",
        "  train_ids = np.hstack([data[t] for t in splits[:7]])\n",
        "  train_ids = [x.split('/')[-1] for x in train_ids]\n",
        "  train_ids = [int(x.split('.')[0]) for x in train_ids]\n",
        "  train = df[df['hadm_id'].isin(train_ids)]['text'].tolist()\n",
        "\n",
        "  train_tagged = []\n",
        "  for idx, text in enumerate(train):\n",
        "      train_tagged.append(TaggedDocument(text, tags=[str(idx)]))\n",
        "\n",
        "  model = Doc2Vec(dm=0, vector_size=200, negative=5, alpha=0.025, hs=0, min_count=5, sample=0, workers=16)\n",
        "  model.build_vocab([x for x in train_tagged])\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      model.train(shuffle([x for x in train_tagged]), total_examples=len(train_tagged), epochs=1)\n",
        "      model.alpha -= 0.0002\n",
        "      model.min_alpha = model.alpha\n",
        "\n",
        "  model.save(f'{models_root}/doc2vec.model')"
      ],
      "metadata": {
        "id": "GI3xWqvxEWGf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **METHODOLOGY (EVALUATION)**\n",
        "\n",
        "We use the patient representation described in the prior section to predict critical clinical outcomes such as in-hospital mortality, 30-day hospital readmission, and extended hospital stays. The effectiveness of the models will demonstrated through evaluations on the MIMIC-III dataset, where they're *claimed* to outperform traditional approaches that rely solely on structured or unstructured data.\n"
      ],
      "metadata": {
        "id": "9VWrw2pmERkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: evaluate trained saved model\n",
        "try:\n",
        "  model = Doc2Vec.load(f'{models_root}/doc2vec.model')\n",
        "  test_ids = np.hstack([data[t] for t in splits[7:]])\n",
        "  test_ids = [x.split('/')[-1] for x in test_ids]\n",
        "  test_ids = [int(x.split('.')[0]) for x in test_ids]\n",
        "  test = df[df['hadm_id'].isin(test_ids)]['text'].tolist()\n",
        "  test_tagged = []\n",
        "  for idx, text in enumerate(test):\n",
        "      test_tagged.append(TaggedDocument(text, tags=[str(idx)]))\n",
        "  vectors = model.infer_vector(test_tagged)\n",
        "except:\n",
        "  print(\"Evaluation pipeline for main model that may need additional preprocessing of test IDs if error running\")\n"
      ],
      "metadata": {
        "id": "tbsKK3hHES5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cef78f6-0b2b-4785-a812-3d7de7391ac0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation pipeline for main model that may need additional preprocessing of test IDs if error running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analyses\n",
        "\n",
        "## Discussion\n",
        "To do before final project submission"
      ],
      "metadata": {
        "id": "52j2CCvN-Oje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PLANS\n",
        "Currently, the codebase of my project is undergoing a significant refactoring to improve maintainability and performance while handling the intricaces of working with both the demo and actual data provided by the MIMIC-III dataset.\n",
        "\n",
        "Although the complete workflow from training to inference is conceptualized in a draft form, it has not yet been fully implemented. The forthcoming stages of my project will involve training the model on the preprocessed  datasets, followed by  testing to evaluate the model’s efficacy.\n",
        "\n",
        "This will be pivotal in validating our hypothesis regarding the model’s performance and potential improvements over existing methods. Upon completion of these steps, I will comprehensively analyze the results and document my findings and conclusions in a detailed report.\n"
      ],
      "metadata": {
        "id": "Lb4T-Zh69pdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REFERENCES\n",
        "\n",
        "1. Zhang, D., Yin, C., Zeng, J. et al. Combining structured and unstructured data for predictive models: a deep learning approach. BMC Med Inform Decis Mak 20, 280 (2020). https://doi.org/10.1186/s12911-020-01297-6\n",
        "\n",
        "2. Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). Dive into deep learning. Cambridge University Press."
      ],
      "metadata": {
        "id": "eIm2JP8o-yew"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46eNnQhB-z-V"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uu4BQMTPwcFK"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}
